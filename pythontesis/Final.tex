\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro

    \usepackage{iftex}
    \ifPDFTeX
        \usepackage[T1]{fontenc}
        \IfFileExists{alphabeta.sty}{
              \usepackage{alphabeta}
          }{
              \usepackage[mathletters]{ucs}
              \usepackage[utf8x]{inputenc}
          }
    \else
        \usepackage{fontspec}
        \usepackage{unicode-math}
    \fi

    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics
                         % to support a larger range
    \makeatletter % fix for old versions of grffile with XeLaTeX
    \@ifpackagelater{grffile}{2019/11/01}
    {
      % Do nothing on new versions
    }
    {
      \def\Gread@@xetex#1{%
        \IfFileExists{"\Gin@base".bb}%
        {\Gread@eps{\Gin@base.bb}}%
        {\Gread@@xetex@aux#1}%
      }
    }
    \makeatother
    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage{array}     % table support for pandoc >= 2.11.3
    \usepackage{calc}      % table minipage width calculation for pandoc >= 2.11.1
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % common color for the border for error outputs.
    \definecolor{outerrorbackground}{HTML}{FFDFDF}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}

    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}


    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{Final}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\@namedef{PY@tok@w}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\@namedef{PY@tok@c}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cp}{\def\PY@tc##1{\textcolor[rgb]{0.61,0.40,0.00}{##1}}}
\@namedef{PY@tok@k}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kt}{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\@namedef{PY@tok@o}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ow}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@nb}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nf}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@nn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@ne}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.80,0.25,0.22}{##1}}}
\@namedef{PY@tok@nv}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@no}{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\@namedef{PY@tok@nl}{\def\PY@tc##1{\textcolor[rgb]{0.46,0.46,0.00}{##1}}}
\@namedef{PY@tok@ni}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@na}{\def\PY@tc##1{\textcolor[rgb]{0.41,0.47,0.13}{##1}}}
\@namedef{PY@tok@nt}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@nd}{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\@namedef{PY@tok@s}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sd}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@si}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@se}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.36,0.12}{##1}}}
\@namedef{PY@tok@sr}{\def\PY@tc##1{\textcolor[rgb]{0.64,0.35,0.47}{##1}}}
\@namedef{PY@tok@ss}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sx}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@m}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@gh}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@gu}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\@namedef{PY@tok@gd}{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\@namedef{PY@tok@gi}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.52,0.00}{##1}}}
\@namedef{PY@tok@gr}{\def\PY@tc##1{\textcolor[rgb]{0.89,0.00,0.00}{##1}}}
\@namedef{PY@tok@ge}{\let\PY@it=\textit}
\@namedef{PY@tok@gs}{\let\PY@bf=\textbf}
\@namedef{PY@tok@gp}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\@namedef{PY@tok@go}{\def\PY@tc##1{\textcolor[rgb]{0.44,0.44,0.44}{##1}}}
\@namedef{PY@tok@gt}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\@namedef{PY@tok@err}{\def\PY@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}}
\@namedef{PY@tok@kc}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kd}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kn}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@kr}{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@bp}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\@namedef{PY@tok@fm}{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\@namedef{PY@tok@vc}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vg}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vi}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@vm}{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\@namedef{PY@tok@sa}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sb}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sc}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@dl}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s2}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@sh}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@s1}{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\@namedef{PY@tok@mb}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mf}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mh}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mi}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@il}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@mo}{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\@namedef{PY@tok@ch}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cm}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cpf}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@c1}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}
\@namedef{PY@tok@cs}{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.24,0.48,0.48}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb.
    \makeatletter
        \newbox\Wrappedcontinuationbox
        \newbox\Wrappedvisiblespacebox
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}}
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}}
        \newcommand*\Wrappedcontinuationindent {3ex }
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox}
        % Take advantage of the already applied Pygments mark-up to insert
        % potential linebreaks for TeX processing.
        %        {, <, #, %, $, ' and ": go to next line.
        %        _, }, ^, &, >, - and ~: stay at end of broken line.
        % Use of \textquotesingle for straight quote.
        \newcommand*\Wrappedbreaksatspecials {%
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}%
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}%
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}%
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}%
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}%
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}%
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}%
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}%
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}%
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}%
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}%
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}%
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}%
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}%
        }
        % Some characters . , ; ? ! / are not pygmentized.
        % This macro makes them "active" and they will insert potential linebreaks
        \newcommand*\Wrappedbreaksatpunct {%
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}%
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}%
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}%
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}%
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}%
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}%
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}%
            \catcode`\.\active
            \catcode`\,\active
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active
            \lccode`\~`\~
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%

        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}

    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        {\ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \hypertarget{optimizaciuxf3n-de-la-soluciuxf3n-de-ecuaciones-diferenciales-a-traves-de-redes-neuronales}{%
\section{OPTIMIZACIÓN DE LA SOLUCIÓN DE ECUACIONES DIFERENCIALES A
TRAVES DE REDES
NEURONALES}\label{optimizaciuxf3n-de-la-soluciuxf3n-de-ecuaciones-diferenciales-a-traves-de-redes-neuronales}}

\hypertarget{resumen}{%
\subsection{Resumen}\label{resumen}}

En el siguiente proyecto se generará la solución de un caso de redes
neuronales con inferencias físicas, posteriormente se analizará el
proceso de entrenamiento de la red y se propondrán maneras de
acelerarlo. Para esto se intentará optimizar la rata de aprendizaje
usada por ADAM y se usaran funciones de distribución de probabilidad
dinámicas.

\hypertarget{orden}{%
\subsection{Orden}\label{orden}}

\begin{itemize}
\tightlist
\item
  Resumen.
\item
  Orden.
\item
  Introducción.
\item
  Metodología
\item
  Explicación de la razón por la cual se están implementando funciones
  de densidad de probabilidad dinámicas.
\item
  Declaración del problema.
\item
  Importación de librerías.
\item
  Declaración de la red neuronal.
\item
  Declaración y explicación del conjunto de puntos sobre el cual se va a
  optimizar la función.
\item
  Declaración de la función de perdida dentro del código.
\item
  Condiciones de parada.
\item
  Serialización de las soluciones y análisis de rata de aprendizaje.
\item
  Análisis de los distintos parámetros de los que depende el algoritmo.
\item
  Resultados de Rata de aprendizaje.
\end{itemize}

\hypertarget{introducciuxf3n}{%
\subsection{Introducción}\label{introducciuxf3n}}

La solución de ecuaciones diferenciales es un reto fundamental para la
ingeniería y la física. Esto se puede ver en la avaluación de compañías
como Ansys o Star CCM+, que se dedican a esto. En los últimos años la
prominencia de las técnicas de inteligencia artificial ha aumentado
significativamente, una pregunta natural ante este escenario es ¿Cómo es
que estas pueden ayudar a solucionar ecuaciones diferenciales?
Respondiendo a esta pregunta surgen las PINNs (Physically Inferred
Neural Networks), cuyo acrónimo significa redes neuronales con
inferencias físicas.

Estas son redes neuronales entrenadas para interpolar o generar
respuestas a ecuaciones diferenciales. Tradicionalmente, las
interpolaciones de las redes neuronales solo tienen en cuenta los
resultados de una simulación o respuesta anterior. Por otro lado, las
simulaciones tradicionales solo tienen en cuenta las condiciones de
frontera. Estas usualmente no usan resultados de simulaciones pasadas ni
experimentos de laboratorio o reglas empíricas. Las PINN por el
contrario, pueden solucionar/interpolar usando resultados de
simulaciones pasadas, experimentos de laboratorios, condiciones de
frontera y/o reglas empíricas.

Esto se debe a que las PINNs usan el descenso de gradiente en reemplazo
de los sistemas de ecuaciones lineales usados por los métodos de
runge-kutta. Esto permite que la inclusión de condiciones al
solucionador, pues no hay una matriz/sistema de ecuaciones para
sobredimensionalizar. Adicionalmente, estas redes usan algoritmos como
la diferenciación automática, que le permite a las redes diferenciar con
respecto a muchas variables con mejor precisión y velocidad que la
diferenciación numérica tradicional.

Desafortunadamente, la solución de ecuaciones diferenciales a través de
PINN resulta altamente costosa computacionalmente para una gran variedad
de implementaciones. Consecuentemente a través del cuaderno se intentará
reducir este costo. Esto se hará a través de un proceso en el que
primero se intenta entender como es que se converge a las respuestas y
luego se intentará optimizar este proceso.

\hypertarget{metodologuxeda-usada.}{%
\subsection{Metodología usada.}\label{metodologuxeda-usada.}}

El proyecto empezó con el objetivo de simular la ecuación de Navier
Stokes para el caso conocido como ``Backwards Flowing Step''. Esto falló
debido a problemas con la implementación del termino de aceleración
local en el módulo de diferenciación automática. Desafortunadamente, el
proceso de solución era demasiado costoso computacionalmente como para
poder iterar nuevas respuestas. Consecuentemente, se intentó solucionar
una ecuación significativamente más simple, optimizarla y aplicar esos
resultados al caso de Navier Stokes.

Durante el proceso de optimización, se consideró que podía ser más
valioso explorar a fondo el efecto de realizar cambios sobre el
entrenamiento de la red. Consecuentemente el objetivo del proyecto
cambió a explorar estos efectos. La ecuación que se decidió resolver fue
la de movimiento armónico simple. Se tomó esta decisión porque la
respuesta de esta está limitada por el intervalo {[}1,-1{]}; es una
ecuación de grado mayor a 1; su respuesta es conocida y su forma es
simple. A continuación, se explicará la importancia de cada una de estas
razones.

Para empezar, se deseaba que la respuesta estuviese dada entre 1 y -1
pues así se podía notar fácilmente si existía algún problema con números
negativos (No hubo, por eso no se menciona en el resto del documento).
Esto podría llegar a ser un problema con otro tipo de función de
activación dentro de la red neuronal como RELU o la función sigmoide
debido a que estas dos funciones solo producen números mayores a 0.

Por otra parte, se quería que la ecuación fuese de grado mayor a 1 para
poder probar el módulo de diferenciación automática. Esto no fue
trivial, pues la implementación de pytorch presentaba inconsistencias a
la hora de optimizar con respecto a variables generadas por un Hessiano.
Consecuentemente se quería comprobar que se pudiese realizar el proyecto
con segundas derivadas.

Continuando con la respuesta conocida y forma simple. Se perdió mucho
tiempo probando e intentando arreglar la función de Navier Stokes.
Consecuentemente, se considero un mejor uso del tiempo usar una función
fácil de probar y arreglar.

Después de programar el entrenamiento de la ecuación diferencial, se
realizo el proceso durante 10 horas en un computador lento e inestable.
Lo anterior incitó a agilizar el proceso. Dado que la solución tomaba
mucho tiempo en converger, se pudieron observar patrones a través de
gráficas que mostraban el progreso de la solución.

Luego de esto, tomando inspiración de la forma en la que los algoritmos
de búsqueda lograron ganar eficiencia a través de minimizar los pasos
innecesarios, se intentó este acercamiento con la optimización de zonas
innecesarias en esta solución. Para esto se implementaron funciones de
densidad de probabilidad dinámicas y se maximizó la tasa de aprendizaje.

\hypertarget{problema}{%
\subsection{Problema}\label{problema}}

Se desea resolver la siguiente ecuación diferencial con las siguientes
condiciones de frontera. \[y'' = y\] \[y(0) = 0\]
\[y(\frac{\pi}{2}) = 1\] En el dominio
\[{x : x\in \mathbb{R} ^ 1 :  x\in [0,7]}\] Residual Sea \(y_r(x)\) la
respuesta de la red neuronal a la entrada \(x\) y \(y_r''(x)\) la
segunda respuesta de la derivada con respecto a x de la red se va a usar
la siguiente función de perdida. Acá los puntos \(x_i\) pertenecen a un
conjunto \(\mathbf{X}\) que será descrito más adelante.
\[L = \sum_{i=1}^n{\Big(w_i * (y_r''(x_i)-y_r(x_i))\Big)^2}+\alpha * (y_r(0)^2+(y_r(\frac{\pi}{2}) - 1)^2)\]

\hypertarget{explicaciuxf3n-de-la-razuxf3n-por-la-cual-se-estuxe1n-implementando-funciones-de-densidad-de-probabilidad-dinuxe1micas.}{%
\subsection{Explicación de la razón por la cual se están implementando
funciones de densidad de probabilidad
dinámicas.}\label{explicaciuxf3n-de-la-razuxf3n-por-la-cual-se-estuxe1n-implementando-funciones-de-densidad-de-probabilidad-dinuxe1micas.}}

Para realizar esto es conveniente hablar de como es que las PINNs
resuelven ecuaciones diferenciales. Esto se hará a través de una
explicación de porque se usan redes neuronales; como es que estas
resuelven ecuaciones diferenciales; el problema generado por el paso
anterior y finalmente como es que se espera que las funciones de
probabilidad de densidad dinámicas lo resuelvan.

Empezando con las redes neuronales, estás no son más que funciones con
una gran cantidad de parámetros que tienen la capacidad de acercarse
mucho a las formas de otras funciones. Esto significa que para una
cantidad adecuada de parámetros/neuronas se debería poder obtener
virtualmente cualquier función deseada. Lo anterior implica que a la
hora de resolver una ecuación diferencial, con una red neuronal que
tenga una cantidad adecuada de parámetros, se podrá obtener una función
que satisface la ecuación diferencial. Adicionalmente, en este caso se
están usando funciones diferenciables como las funciones de activación
(Parte de las neuronas). En resumen, se están usando redes neuronales
porque se espera que estas tengan la capacidad de tomar la forma de la
respuesta a la ecuación diferencial.

La pregunta que surge naturalmente ante el parrado anterior es ¿Cómo
lograr que la red tome la forma de la función deseada? Esto se realiza
mediante del uso del aprendizaje por descenso de gradiente estocástico.
Este es el nombre de una familia de algoritmos que resuelve el problema
a través de variar una gran cantidad de parámetros dentro de la red.
Para realizar esto el algoritmo utiliza una función (en este caso la
función \(L\)) que le indique que tan bien funciona cada combinación de
parámetros. De está forma el algoritmo llega a los parámetros que
maximicen la función anterior.

La forma tradicional para realizar lo anteriormente descrito consiste
en: 1. Seleccionar un conjunto de puntos aleatorios que pertenezcan al
dominio donde se espera resolver la ecuación diferencial. 2. Evaluar que
tan bien estos puntos cumplen la ecuación diferencial. 3. Evaluar que
tan bien se cumplen las condiciones de frontera. 4. Actualizar los
parámetros para mejorar los puntajes de los puntos 2 y 3 a través de
descenso de gradiente. 5. Repetir el paso 1.

Un aspecto fundamental del proceso es que para cada punto sobre el que
se está optimizando, el punto solo va a intentar mejorar basándose en la
información de la ecuación diferencial. Esto significa que en este caso
especifico, para cualquier punto \(x_i\) este solo va a intentar que la
concavidad sea igual a \(y\) es decir que \(y''(x_i) = y(x_i)\). Lo
anterior significa que a este punto no le importa la condición de
frontera del problema sino su valor actual de \(y\) y \(y''\) pues la
red neuronal solo quiere igualar a esos valores. Para que el punto
llegue a la solución deseada, los puntos con un valor de x un poco
menores deben tener los valores de \(y,y'\) y \(y''\) correctos. Esto
significa que la solución se va a propagar desde las condiciones de
frontera al resto del dominio.

Lo anterior se puede ver desde una serie de Taylor y una inducción.
Suponiendo que se tienen infinitos puntos separados por distancias muy
pequeñas. Si para un punto, el valor de la función y las derivadas es
correcto, según las expansiones de Taylor, el valor de \(y\) del
siguiente punto será correcto. Consecuentemente, si las derivadas de
este punto son correctas, se espera que el siguiente punto sea correcto.
Dado que el punto puesto sobre la condición de frontera va a tener el
valor de \(y\) apropiado, se espera que después de ajustar las derivadas
acordes a la ecuación diferencial, el siguiente a este se llegue al
valor de \(y\) correcto. Después de esto se espera que este lleve al
siguiente a la posición correcta y que este proceso se propague hasta
que se resuelva la ecuación diferencial.

Para explicar el problema asociado al procedimiento listado
anteriormente, hay que tomar en cuenta que una gran mayoría de las
ecuaciones diferenciales usadas son homogéneas y tienen una cantidad
infinita de respuestas posibles (la cantidad de respuestas en este caso
es del tamaño del conjunto \(\mathbb{R}^2\) porque la ecuación
diferencial es de grado 2). desafortunadamente esto lleva a que existan
muchas respuestas que resuelven la ecuación diferencial pero no las
condiciones de frontera. Ejemplos de esto son \$y(x) = 0 \text{ ; } y(x)
= cos (x) \text{ ; } y(x) = 0.1 sin(x) \$. Lo anterior significa que se
necesita resolver la ecuación diferencial que resuelva las condiciones
de frontera.

El método tradicional primero se acopla a las condiciones de frontera y
luego propaga la ecuación diferencial al resto del dominio. Esto
significa que no todas las partes del dominio se van a resolver de
manera simultanea. Las regiones más cercanas a las condiciones de
frontera se van a resolver antes que el resto. Después de que estas se
resuelvan, debido a que se están usando funciones diferenciables,
eventualmente se va a propagar la solución correcta al resto del
dominio.

Lo anterior significa que si se están colocando puntos sobre las zonas
en las cuales la solución no se ha propagado, se está perdiendo el
tiempo y esfuerzo computacional. Consecuentemente se espera que al
utilizar una función de densidad de probabilidad que no coloque los
puntos de manera uniformemente repartida, sino solo cerca a las
condiciones de frontera, se logre aumentar la velocidad de convergencia.

    \hypertarget{importaciuxf3n-de-las-libreruxedas}{%
\subsection{Importación de las
librerías}\label{importaciuxf3n-de-las-libreruxedas}}

Se usan las siguientes librerías por las siguientes razones

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.50}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.50}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
librería Usada
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Razón
\end{minipage} \\
\midrule
\endhead
Torch & Ofrece documentación más accesible que Julia y tiene modos de
diferenciación automatica que facilitan el trabajo \\
torch.nn & Con este modulo se declara la clase con la que se genera la
red neuronal \\
matplotlib & Graficación \\
random & Generación de puntos con una distribución Normal \\
time & Seguimiento del tiempo tomado por cada proceso \\
Typing & Generación de listas con tipos de datos estáticos \\
pickle & serialización y guardado de variables \\
OS & manejo de carpetas \\
\bottomrule
\end{longtable}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k+kn}{import} \PY{n+nn}{torch} 
\PY{k+kn}{import} \PY{n+nn}{torch}\PY{n+nn}{.}\PY{n+nn}{nn} \PY{k}{as} \PY{n+nn}{nn}
\PY{k+kn}{import} \PY{n+nn}{matplotlib}\PY{n+nn}{.}\PY{n+nn}{pyplot} \PY{k}{as} \PY{n+nn}{plt}
\PY{k+kn}{import} \PY{n+nn}{random} \PY{k}{as} \PY{n+nn}{rd}
\PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
\PY{k+kn}{import} \PY{n+nn}{time}
\PY{k+kn}{from} \PY{n+nn}{typing} \PY{k+kn}{import} \PY{n}{List}
\PY{k+kn}{import} \PY{n+nn}{pickle}
\PY{k+kn}{import} \PY{n+nn}{os} 
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{declaraciuxf3n-de-la-red-neuronal}{%
\subsection{Declaración de la red
neuronal}\label{declaraciuxf3n-de-la-red-neuronal}}

Se va a usar una red normal con 5 capas de 50 neuronas. Esto se eligió
basandose en la literatura.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{os}\PY{o}{.}\PY{n}{chdir}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{/home/externo/Documents/nico/efficientPINN/pythontesis}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{os}\PY{o}{.}\PY{n}{chdir}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{v6}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{c+c1}{\PYZsh{} Esta linea hace que el cuaderno corra sobre el directorio deseado}
\PY{n}{device} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cuda}\PY{l+s+s2}{\PYZdq{}} \PY{c+c1}{\PYZsh{} Esta linea hace que el cuaderno se ejecute con la gpu de envidia}
\PY{n}{dtype} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{float32} \PY{c+c1}{\PYZsh{} Esta linea hace que el tipo de dato sean floats de 64 bits}
\PY{n}{estaResolviendo} \PY{o}{=} \PY{k+kc}{True} \PY{c+c1}{\PYZsh{} Esta variable se usa para controlar si se quiere que se resuelvan las redes neuronales.}
\PY{c+c1}{\PYZsh{} Se pone en false si no se desea resolver nada.}
\PY{n}{buscandoLearningRate} \PY{o}{=} \PY{k+kc}{False} \PY{c+c1}{\PYZsh{} Esta variable se usa para controlar si se requiere hacer el analisis de learning rate.}
\PY{c+c1}{\PYZsh{} como este analisis toma mucho tiempo, el default es falso}
\PY{n}{serializar} \PY{o}{=} \PY{k+kc}{True}

\PY{k}{class} \PY{n+nc}{NeuralNetworkPrueba}\PY{p}{(}\PY{n}{nn}\PY{o}{.}\PY{n}{Module}\PY{p}{)}\PY{p}{:} \PY{c+c1}{\PYZsh{} Acá se declara el tipo de red que se va a usar}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{)}\PY{p}{:}
        \PY{n+nb}{super}\PY{p}{(}\PY{n}{NeuralNetworkPrueba}\PY{p}{,} \PY{n+nb+bp}{self}\PY{p}{)}\PY{o}{.}\PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{flatten} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Flatten}\PY{p}{(}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{linear\PYZus{}relu\PYZus{}stack} \PY{o}{=} \PY{n}{nn}\PY{o}{.}\PY{n}{Sequential}\PY{p}{(}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Tanh}\PY{p}{(}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Tanh}\PY{p}{(}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Tanh}\PY{p}{(}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Tanh}\PY{p}{(}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{50}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Tanh}\PY{p}{(}\PY{p}{)}\PY{p}{,}
            \PY{n}{nn}\PY{o}{.}\PY{n}{Linear}\PY{p}{(}\PY{l+m+mi}{50}\PY{p}{,} \PY{l+m+mi}{1}\PY{p}{)}
        \PY{p}{)}

    \PY{k}{def} \PY{n+nf}{forward}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,} \PY{n}{x}\PY{p}{)}\PY{p}{:}
        \PY{n}{logits} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{linear\PYZus{}relu\PYZus{}stack}\PY{p}{(}\PY{n}{x}\PY{p}{)}
        \PY{k}{return} \PY{n}{logits}

\PY{n}{redDinamica} \PY{o}{=} \PY{n}{NeuralNetworkPrueba}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)} \PY{c+c1}{\PYZsh{} Acá se declara la función de la red neuronal y se pone a correr en gpu}
\PY{n}{redDinamica}\PY{o}{.}\PY{n}{float}\PY{p}{(}\PY{p}{)}
\PY{c+c1}{\PYZsh{}redDinamicaCPU = NeuralNetworkPrueba().to(\PYZdq{}\PYZdq{})}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{2}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
NeuralNetworkPrueba(
  (flatten): Flatten(start\_dim=1, end\_dim=-1)
  (linear\_relu\_stack): Sequential(
    (0): Linear(in\_features=1, out\_features=50, bias=True)
    (1): Tanh()
    (2): Linear(in\_features=50, out\_features=50, bias=True)
    (3): Tanh()
    (4): Linear(in\_features=50, out\_features=50, bias=True)
    (5): Tanh()
    (6): Linear(in\_features=50, out\_features=50, bias=True)
    (7): Tanh()
    (8): Linear(in\_features=50, out\_features=50, bias=True)
    (9): Tanh()
    (10): Linear(in\_features=50, out\_features=1, bias=True)
  )
)
\end{Verbatim}
\end{tcolorbox}
        
    \hypertarget{declaraciuxf3n-y-explicaciuxf3n-del-conjunto-de-puntos-sobre-el-cual-se-va-a-optimizar-la-funciuxf3n.}{%
\subsection{Declaración y explicación del conjunto de puntos sobre el
cual se va a optimizar la
función.}\label{declaraciuxf3n-y-explicaciuxf3n-del-conjunto-de-puntos-sobre-el-cual-se-va-a-optimizar-la-funciuxf3n.}}

\hypertarget{orden-interno}{%
\subsubsection{Orden interno}\label{orden-interno}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Introducción y justificación para la existencia del algoritmo
\item
  Variables usadas en el algoritmo
\item
  Funciones usadas en el algoritmo
\item
  Algoritmo
\item
  Explicación del algoritmo
\end{enumerate}

\hypertarget{introducciuxf3n-y-justificaciuxf3n-para-la-existencia-del-algoritmo-para-la-colocaciuxf3n-de-puntos}{%
\subsubsection{Introducción y justificación para la existencia del
algoritmo para la colocación de
puntos}\label{introducciuxf3n-y-justificaciuxf3n-para-la-existencia-del-algoritmo-para-la-colocaciuxf3n-de-puntos}}

A la hora de resolver la ecuación diferencial mediante PINNs hay que
escoger un conjunto de puntos sobre el cual se va a optimizar la
función. El acercamiento predeterminado para PINNs es escoger varios
sets de puntos aleatorios en el dominio. No obstante, en experimentación
previa se noto que esto es un proceso poco eficiente. Esto se debe a que
el optimizador va a intentar llevar los puntos hacia la solución de la
ecuación diferencial más cercana. Lo anterior significa que en las
etapas iniciales de la optimización los puntos lejanos a las condiciones
de frontera van a converger a soluciones que si bien pueden pertenecer
al conjunto de soluciones de la ecuación diferencial, no son la ecuación
diferencial que se necesita. Esto lleva a que se este optimizando
parámetros para llegar a una forma inútil. Con el objetivo de eliminar
este problema se propone el siguiente método

Para resolver el problema anterior se propone un método en el cual no se
resuelven los puntos que estén alejados de las condiciones de frontera
hasta que la frontera de lo que está resuelto de la ecuación diferencial
se acerque a ellos. Para lograr esto se propone empezar con puntos que
estén cercanos a la condición de frontera inicial e ir moviendo estos
puntos hacia el resto del dominio. De esta forma se espera que todos los
puntos converjan a la solución esperada y no a otra posible solución de
la ecuación diferencial. A continuación se detalla el algoritmo para la
selección De estos puntos.

\hypertarget{variables-usadas-en-el-algoritmo-para-la-colocaciuxf3n-de-puntos}{%
\subsubsection{Variables usadas en el algoritmo para la colocación de
puntos}\label{variables-usadas-en-el-algoritmo-para-la-colocaciuxf3n-de-puntos}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.33}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.33}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.33}}@{}}
\toprule
\begin{minipage}[b]{\linewidth}\raggedright
Variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
valor Inicial
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Interpretación
\end{minipage} \\
\midrule
\endhead
\(\mu\) & 0 & Es el punto en el cual se encuentra el solucionador, es
una variable que varia mientras avanza el solucionador \\
\(\sigma\) & 0.2 & Es la desviación estándar con la cual se ponen puntos
cerca a la frontera definida por \(\mu\) \\
\(n_{max}\) & 200 & Es el largo máximo que puede tener el conjunto
\(\mathbf{X}\) \\
\(n_{ret}\) & 60 & El numero de puntos que se borran del conjunto
\(\mathbf{X}\) \\
\(n_{pas}\) & 50 & El numero de puntos que se añaden al conjunto
\(\mathbf{X}\) \\
\(n_{ini}\) & 20 & Numero de puntos con los cuales se inicializa el
conjunto \(\mathbf{X}\) \\
\(c_1\) & \(10^{-6}\) & Un coeficiente que define linealmente que tan
rapido se avanza con respecto a la rata de solución \\
\(c_2\) & 1 & Un criterio para definir la agresividad con la cual se
quiere avanzar con respecto a la solución \\
\(L_i\) & dinámico & Valor de la perdida obtenida en la iteración i \\
\(x_{inf}\) & 0 & Limite inferior del dominio sobre ,el cual se va a
resolver la ecuación diferencial \\
\(x_{sup}\) & 0 & Limite superior del dominio sobre ,el cual se va a
resolver la ecuación diferencial \\
\bottomrule
\end{longtable}

\hypertarget{funciones-usadas-en-el-algoritmo-para-la-colocaciuxf3n-de-puntos}{%
\subsubsection{Funciones usadas en el algoritmo para la colocación de
puntos}\label{funciones-usadas-en-el-algoritmo-para-la-colocaciuxf3n-de-puntos}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Funcion limitadora del dominio:

  Esta es una función hecha para que si el algoritmo recorre todo el
  dominio sin converger, este pueda volver al punto inicial
  \(l : \mathbb{R}\rightarrow \mathbb{R}.\)
  \[ l(x) = mod(x_{sup}-x_{inf},x-x_{inf})+x_{inf} \]
\item
  relación generadora de puntos f (\$\mu \(,\)\sigma\$) retorna un punto
  aleatorio con distribución normal de promedio \(\mu\) y desviación
  estandar \$\sigma \$
\end{enumerate}

\hypertarget{algoritmo-para-la-colocaciuxf3n-de-puntos}{%
\subsubsection{Algoritmo para la colocación de
puntos}\label{algoritmo-para-la-colocaciuxf3n-de-puntos}}

Se inicializan los siguientes valores en
\(\mu _0 = 0, \sigma = 0.05, c_1 = 1e-5, c_2 = 1\). Luego de esto se
inicializa un conjunto con \(n_{ini}\) puntos repartidos uniformemente a
través del dominio. Estos puntos solo están para asegurar que la perdida
sea diferente de 0 en el primer momento de la optimización. Al final del
documento se elige un set diferente de estos numeros y se justifica
mediante de un proceso de optimización.

Despues de esto para cada momento i+1 en el tiempo 1. Se define \$
\mu\emph{\{i+1\} = \mu}\{i\} + \frac{c_1}{L_i^{c_2}} \$ 2. Se genera un
punto nuevo tal que \$ x\_\{i+1\} = l(g(\mu\_\{i+1\},\sigma))\$ 3. Se
anade el elemento \(x_{i+1}\) al conjunto \(\mathbf{X}\) 4. Si
\(|\mathbf{X}|>n_{max}\) aleatoriamente se retiran \(n_{ret}\) elementos
del conjunto. 5. Si se retiran elementos del conjunto, Se añaden
\(n_{pas}\) elementos al conjunto. Cada uno de estos elementos se elige
como un numero aleatorio entre 0 y \(\mu_{i+1}\) 6. Se guarda el valor
de \(L(x_{i+1})\)

\hypertarget{explicaciuxf3n-de-los-pasos-del-algoritmo}{%
\subsubsection{Explicación de los pasos del
algoritmo}\label{explicaciuxf3n-de-los-pasos-del-algoritmo}}

Para empezar hay que tener en cuenta que el algoritmo divide sobre el
valor de la perdida. Para evitar que este valor empiece en 0, se eligen
20 puntos repartidos aleatoriamente dentro del dominio. Después de esto,
se va a empezar el proceso de añadir puntos. Un indicador de que tan
bien se esta resolviendo la ecuación diferencial es la función de
perdida. Consecuentemente se quiere que los puntos avancen cuando
disminuya la función de perdida. Por esto se tomo
\(\Delta \mu = \frac{c_1}{L^c_2}\) .

Desafortunadamente el algoritmo hasta ese punto aumenta
significativamente la complejidad computacional en el tiempo del
problema. Esto se debe a que la velocidad de las épocas de entrenamiento
depende linealmente del largo del conjunto. Consecuentemente no es ideal
aumentar el largo del conjunto de forma indefinida y proporcional a las
épocas de entrenamiento. Es por esto que en el paso 4 se introduce un
filtro tal que el tamaño del conjunto sea limitado.

Este filtro no es un filtro tan convencional, se eliminan más puntos de
los que se añaden. Esto se debe a que si se añadiera la misma cantidad
de puntos que se elimina, en promedio la cantidad de puntos cerca a la
frontera se reducirían en una proporción \(n_{ret}/n_{max}\) con cada
iteración. A través de experimentación se noto que estos puntos se
reducían de manera excesivamente rápida. Esto es resuelto haciendo que
después del filtro el tamaño del conjunto sea unas unidades (10 unidades
funciona) menor al limite impuesto por \(n_{max}\). De esta forma la
cantidad de puntos cerca a la frontera se reducirán en una proporción
menor. Esta proporción la indica la ecuación a continuación.

Después de una reducción de puntos del paso 4 en la iteración i+1 se
espera que la proporción de puntos cerca a la frontera sea reducida.
Para expresar esto, la cantidad de puntos cercanos a la frontera serán
representados por la variable \(\eta_{i+1}\). Estos se reduzca de la
siguiente forma:
\[\frac{\eta_{i+1}}{\eta_{i}} = \frac{n_{ret}}{n_{max}}\] No obstante,
dado que este paso solo ocurre cada \(n_{ret} - n_{pas}\)
actualizaciones, el cambio esperado por la proporción de puntos estaría
dado por la siguiente formula.

\[\eta_{i+n_{ret} - n_{pas}} = n_{ret} - n_{pas} + \frac{n_{ret}}{n_{max}} * \eta_{i}\]

De esta forma el limite cuando i tiende a infinito, el numero de puntos
cerca a la frontera no tiende a 0.

Adicionalmente, se están añadiendo puntos nuevos en zonas que en teoría
ya fueron resueltas. Si bien este acercamiento parecería reducir la
eficiencia del algoritmo, hay que tomar en cuenta que el problema esta
siendo resuelto mediante métodos de descenso de gradiente.
Consecuentemente con el objetivo de que al resolver puntos actuales no
se eliminen los resultados anteriores, se colocan puntos en las
soluciones anteriores. De esta forma, cuando se altere la respuesta de
la red en puntos anteriores, el descenso de gradiente las devolverá a la
solución anterior. Finalmente, en estos puntos la variable \(w_i\)
dentro de la sumatoria toma un valor de 20, de esta forma, estos puntos
pesan 20 veces más que el resto de los puntos dentro de la función de
perdida. Lo anterior les da prioridad a estos puntos dentro de la
función.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{3}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{puntos} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{7}\PY{p}{,}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{device} \PY{o}{=} \PY{n}{device}\PY{p}{)} \PY{c+c1}{\PYZsh{}Inicialización de los puntos de prueba}
\PY{n}{puntos}\PY{o}{.}\PY{n}{requires\PYZus{}grad} \PY{o}{=} \PY{k+kc}{True} \PY{c+c1}{\PYZsh{} Atributo que indica que estos puntos se usan para la optimización }
\PY{n}{perdidavar} \PY{o}{=} \PY{l+m+mf}{1e9} \PY{c+c1}{\PYZsh{}Variable posicionada acá para que quede fuera del espacio de memoria de las funciones siguientes}
\PY{n}{promAct} \PY{o}{=} \PY{l+m+mi}{0}  \PY{c+c1}{\PYZsh{} Lo mismo que arriba}
\PY{n}{xsup} \PY{o}{=} \PY{l+m+mi}{7} \PY{c+c1}{\PYZsh{} limite superior del dominio}

\PY{k}{def} \PY{n+nf}{limitador}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{p}{:}
    \PY{n}{xint} \PY{o}{=} \PY{n}{x}
    \PY{k}{while} \PY{p}{(}\PY{n}{xint}\PY{o}{\PYZgt{}}\PY{n}{xsup}\PY{p}{)}\PY{p}{:}
        \PY{n}{xint} \PY{o}{\PYZhy{}}\PY{o}{=} \PY{n}{xsup}
    \PY{k}{return} \PY{n}{xint}
    
\PY{n}{c2} \PY{o}{=} \PY{l+m+mi}{1}
\PY{n}{c1} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}6} \PY{c+c1}{\PYZsh{}este coeficiente es para alentizar el avance con respecto a la función de perdiad}
\PY{n}{sigma} \PY{o}{=} \PY{l+m+mf}{0.2}

\PY{k}{def} \PY{n+nf}{inicializarPuntos}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{k}{global} \PY{n}{puntos}\PY{p}{,}\PY{n}{promAct}
    \PY{n}{puntos} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{20}\PY{p}{,} \PY{n}{device} \PY{o}{=} \PY{n}{device}\PY{p}{)}
    \PY{n}{pesos} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{l+m+mi}{20}
    \PY{n}{promAct} \PY{o}{=} \PY{l+m+mf}{0.1}

\PY{k}{def} \PY{n+nf}{actualizarPuntos}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} Esta función va actualizar los puntos. Para mantener un balance entre puntos nuevos y puntos anteriores}
    \PY{c+c1}{\PYZsh{}  tambien va a cubir el espacio recorrido por puntos anteriores. Un objetivo de esta función es correr }
    \PY{c+c1}{\PYZsh{} con un numero bajo de puntos}
    \PY{k}{global} \PY{n}{perdidavar}\PY{p}{,}\PY{n}{puntos}\PY{p}{,}\PY{n}{puntosCreados}\PY{p}{,}\PY{n}{promAct}\PY{p}{,}\PY{n}{c1}\PY{p}{,}\PY{n}{c2}\PY{p}{,}\PY{n}{sigma}
    
    \PY{n}{nuevosPuntos} \PY{o}{=} \PY{n}{puntos}\PY{o}{.}\PY{n}{detach}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}
    \PY{n}{promAct} \PY{o}{=} \PY{n}{limitador}\PY{p}{(}\PY{n}{promAct}\PY{o}{+}\PY{n}{c1}\PY{o}{/}\PY{p}{(}\PY{n}{perdidavar}\PY{o}{*}\PY{n+nb}{len}\PY{p}{(}\PY{n}{puntos}\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{n}{c2}\PY{p}{)}  \PY{c+c1}{\PYZsh{} acá se da el avance en promedio}
    \PY{c+c1}{\PYZsh{}print(\PYZdq{}promedio actual \PYZdq{}, promAct)}
    \PY{c+c1}{\PYZsh{}print(\PYZdq{}perdida actual  \PYZdq{}, perdidavar)}
    \PY{c+c1}{\PYZsh{}print(\PYZdq{}avance          \PYZdq{}, coeficiente/perdidavar*len(puntos))}
    \PY{n}{nuevoPunto} \PY{o}{=} \PY{n+nb}{max}\PY{p}{(}\PY{n+nb}{min}\PY{p}{(}\PY{n}{rd}\PY{o}{.}\PY{n}{normalvariate}\PY{p}{(}\PY{n}{promAct}\PY{p}{,}\PY{n}{sigma}\PY{p}{)}\PY{p}{,}\PY{n}{xsup}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)} \PY{c+c1}{\PYZsh{} acá se da el avance como una distribución normal}
    \PY{n}{nuevosPuntos}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{nuevoPunto}\PY{p}{)}    
    \PY{n}{tamTensor} \PY{o}{=} \PY{l+m+mi}{120}
    \PY{n}{borra} \PY{o}{=} \PY{l+m+mi}{60}
    \PY{n}{anade} \PY{o}{=} \PY{l+m+mi}{50}
    \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{nuevosPuntos}\PY{p}{)}\PY{o}{\PYZgt{}}\PY{n}{tamTensor}\PY{p}{:}
        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{borra}\PY{p}{)}\PY{p}{:}
            \PY{n}{indice} \PY{o}{=} \PY{n}{rd}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{tamTensor}\PY{o}{\PYZhy{}}\PY{n}{borra}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)}
            \PY{n}{nuevosPuntos}\PY{o}{.}\PY{n}{pop}\PY{p}{(}\PY{n}{indice}\PY{p}{)}
        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{anade}\PY{p}{)}\PY{p}{:}
            \PY{n}{nuevosPuntos}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{rd}\PY{o}{.}\PY{n}{uniform}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{promAct}\PY{p}{)}\PY{p}{)}
    \PY{n}{puntos} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{nuevosPuntos}\PY{p}{,}\PY{n}{device} \PY{o}{=} \PY{n}{device}\PY{p}{)}
    \PY{n}{puntos}\PY{o}{.}\PY{n}{requires\PYZus{}grad} \PY{o}{=} \PY{k+kc}{True}

\PY{n}{pesos} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}\PY{o}{*}\PY{l+m+mi}{20}
\PY{k}{def} \PY{n+nf}{actualizarPuntosConPesos}\PY{p}{(}\PY{p}{)}\PY{p}{:}
     \PY{c+c1}{\PYZsh{} Esta función va actualizar los puntos. Para mantener un balance entre puntos nuevos y puntos anteriores}
    \PY{c+c1}{\PYZsh{}  tambien va a cubir el espacio recorrido por puntos anteriores. Un objetivo de esta función es correr }
    \PY{c+c1}{\PYZsh{} con un numero bajo de puntos}
    \PY{k}{global} \PY{n}{perdidavar}\PY{p}{,}\PY{n}{puntos}\PY{p}{,}\PY{n}{puntosCreados}\PY{p}{,}\PY{n}{promAct}\PY{p}{,}\PY{n}{c1}\PY{p}{,}\PY{n}{c2}\PY{p}{,}\PY{n}{sigma}\PY{p}{,} \PY{n}{pesos}
    \PY{k}{if} \PY{n+nb}{isinstance}\PY{p}{(}\PY{n}{pesos}\PY{p}{,}\PY{n}{List}\PY{p}{)}\PY{p}{:}
        \PY{n}{nuevosPesos} \PY{o}{=} \PY{n}{pesos}\PY{o}{.}\PY{n}{copy}\PY{p}{(}\PY{p}{)}
    \PY{k}{else}\PY{p}{:}
        \PY{n}{nuevosPesos} \PY{o}{=} \PY{n}{pesos}\PY{o}{.}\PY{n}{detach}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}
    \PY{n}{nuevosPuntos} \PY{o}{=} \PY{n}{puntos}\PY{o}{.}\PY{n}{detach}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{tolist}\PY{p}{(}\PY{p}{)}
    \PY{n}{promAct} \PY{o}{=} \PY{n}{limitador}\PY{p}{(}\PY{n+nb}{min}\PY{p}{(}\PY{n}{promAct}\PY{o}{+}\PY{n}{c1}\PY{o}{/}\PY{p}{(}\PY{n}{perdidavar}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{puntos}\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{n}{c2}\PY{p}{,}\PY{n}{promAct}\PY{o}{+}\PY{l+m+mf}{0.3}\PY{p}{)}\PY{p}{)}  \PY{c+c1}{\PYZsh{} acá se da el avance en promedio}
    \PY{n}{nuevoPunto} \PY{o}{=} \PY{n+nb}{max}\PY{p}{(}\PY{n+nb}{min}\PY{p}{(}\PY{n}{rd}\PY{o}{.}\PY{n}{normalvariate}\PY{p}{(}\PY{n}{promAct}\PY{p}{,}\PY{n}{sigma}\PY{p}{)}\PY{p}{,}\PY{n}{xsup}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)} \PY{c+c1}{\PYZsh{} acá se da el avance como una distribución normal}
    \PY{n}{nuevosPuntos}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{nuevoPunto}\PY{p}{)} 
    \PY{n}{nuevosPesos}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}   
    \PY{n}{tamTensor} \PY{o}{=} \PY{l+m+mi}{40}
    \PY{n}{borra} \PY{o}{=} \PY{l+m+mi}{30}
    \PY{n}{anade} \PY{o}{=} \PY{l+m+mi}{20}
    \PY{k}{if} \PY{n+nb}{len}\PY{p}{(}\PY{n}{nuevosPuntos}\PY{p}{)}\PY{o}{\PYZgt{}}\PY{n}{tamTensor}\PY{p}{:}
        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{borra}\PY{p}{)}\PY{p}{:}
            \PY{n}{indice} \PY{o}{=} \PY{n}{rd}\PY{o}{.}\PY{n}{randint}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{tamTensor}\PY{o}{\PYZhy{}}\PY{n}{borra}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{)}
            \PY{n}{nuevosPuntos}\PY{o}{.}\PY{n}{pop}\PY{p}{(}\PY{n}{indice}\PY{p}{)}
            \PY{n}{nuevosPesos}\PY{o}{.}\PY{n}{pop}\PY{p}{(}\PY{n}{indice}\PY{p}{)}
        \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n}{anade}\PY{p}{)}\PY{p}{:}
            \PY{n}{nuevosPuntos}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{rd}\PY{o}{.}\PY{n}{uniform}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n}{promAct}\PY{p}{)}\PY{p}{)}
            \PY{n}{nuevosPesos}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{l+m+mi}{20}\PY{p}{)}
    \PY{n}{puntos} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{n}{nuevosPuntos}\PY{p}{,}\PY{n}{device} \PY{o}{=} \PY{n}{device}\PY{p}{,}\PY{n}{dtype}\PY{o}{=}\PY{n}{dtype}\PY{p}{)}
    \PY{n}{puntos}\PY{o}{.}\PY{n}{requires\PYZus{}grad} \PY{o}{=} \PY{k+kc}{True}
    \PY{n}{pesos} \PY{o}{=} \PY{n}{nuevosPesos}
    \PY{k}{if} \PY{n+nb}{type}\PY{p}{(}\PY{n}{promAct}\PY{p}{)} \PY{o}{==} \PY{n}{torch}\PY{o}{.}\PY{n}{Tensor}\PY{p}{:}
        \PY{n}{promAct} \PY{o}{=} \PY{n}{promAct}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{declaraciuxf3n-de-la-funciuxf3n-de-perdida-dentro-del-cuxf3digo}{%
\subsection{Declaración de la función de perdida dentro del
código}\label{declaraciuxf3n-de-la-funciuxf3n-de-perdida-dentro-del-cuxf3digo}}

\hypertarget{orden}{%
\subsubsection{Orden}\label{orden}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Problema a Resolver
\item
  Notas de la función de perdida
\end{enumerate}

\hypertarget{problema}{%
\subsubsection{Problema}\label{problema}}

\hypertarget{este-problema-fue-copiado-y-pegado-de-la-secciuxf3n-inicial}{%
\paragraph{Este problema fue copiado y pegado de la sección
inicial}\label{este-problema-fue-copiado-y-pegado-de-la-secciuxf3n-inicial}}

Se desea resolver la siguiente ecuación diferencial con las siguientes
condiciones de frontera. \[y'' = y\] \[y(0) = 0\]
\[y(\frac{\pi}{2}) = 1\] En el dominio
\[{x : x\in \mathbb{R} ^ x\in {0,7}}\] Residual Sea \(y_r(x)\) la
respuesta de la red neuronal a la entrada x y \(y_r''(x)\) la segunda
respuesta de la derivada con respecto a x de la red se va a usar la
siguiente función de perdida. Acá los puntos \(x_i\) pertenecen al
conjunto \(\mathbf{X}\).
\[L = \sum_{i=1}^n{(y_r''(x_i)-y_r(x_i))^2}+\alpha * (y_r(0)^2+(y_r(\frac{\pi}{2}) - 1)^2)\]

\hypertarget{notas-de-la-funciuxf3n-de-perdida.}{%
\subsubsection{Notas de la función de
perdida.}\label{notas-de-la-funciuxf3n-de-perdida.}}

Para empezar hay que notar que se están usando técnicas de aprendizaje
profundo. En general estas técnicas dependen de optimizar una función
con una gran cantidad de parámetros a través de su gradiente. Para
evitar que esto sea un problema significativo en términos de complejidad
computacional se usa un algoritmo de diferenciación automática para que
la complejidad de generar el vector gradiente no dependa de la dimensión
del vector gradiente.

Lo anterior significa que se tiene que usar una implementación que tenga
en cuenta la diferenciación automática, para esto se usa Pytorch. El
requisito que pide esta librería es que se indiquen cuales variables van
a ser necesitadas a la hora de la diferenciación. Estas están
identificadas con el atributo requires\_grad = True

\hypertarget{paralelizaciuxf3n}{%
\subsubsection{Paralelización}\label{paralelizaciuxf3n}}

La función que genera calcula la perdida debería de correr en tiempo
\(O(1)\) para ciertos \(n_{max}\), pues esta corriendo en GPU y se
supone que hay al menos 3000 núcleos libres, este no es el caso. Debido
a la restricción GIL de Python, que no permite utilizar más de un hilo
en simultaneo, no fue posible lograr que la esta función corra
adecuadamente. Se intento utilizar torchscript de forma extensiva, este
es un sublenguaje de python que corre en C y en teoría podía
paralelizar. Desafortunadamente este lenguaje no pudo mantener los
arboles asociados a la diferenciación avanzada para las derivadas de
segundo grado. Consecuentemente fue utilizada una versión sin
paralelismo que corría en tiempo \(O(n)\).

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{perdida2}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{k}{global} \PY{n}{perdidavar} \PY{c+c1}{\PYZsh{} esta variable guarda la perdida en una ubicación afuera de la función.}
    \PY{n}{x0} \PY{o}{=}  \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.0}\PY{p}{]}\PY{p}{,}\PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{,}\PY{n}{requires\PYZus{}grad} \PY{o}{=}\PY{k+kc}{True}\PY{p}{)} \PY{c+c1}{\PYZsh{} Ubicación en X de la primera condición de frontera}
    \PY{c+c1}{\PYZsh{} el 0.0 es importante para que pytorch lo identifique como un float.}
    \PY{c+c1}{\PYZsh{} acá el argumento device se usa para indicar que el vector debe guardarse en la memoria de la gpu}
    \PY{c+c1}{\PYZsh{} Acá el rgumento requires\PYZus{}grad = true se usa para indicar que la variable importa a la hora de la diferenciación }
    \PY{n}{xPi} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mf}{3.14159}\PY{o}{/}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{n}{device} \PY{o}{=} \PY{n}{device}\PY{p}{)} 
    \PY{n}{xPi} \PY{o}{=}  \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{p}{[}\PY{n}{xPi}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{,}\PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{,}\PY{n}{requires\PYZus{}grad} \PY{o}{=}\PY{k+kc}{True}\PY{p}{)} \PY{c+c1}{\PYZsh{} Ubicación en X se la segunda condición de frontera}
    \PY{n}{suma} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n}{puntos}\PY{p}{:}
        \PY{n}{i} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{,}\PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{,}\PY{n}{requires\PYZus{}grad} \PY{o}{=}\PY{k+kc}{True}\PY{p}{)} 
        \PY{c+c1}{\PYZsh{} hay que meter el valor en X dentro de un vector para que pytorch lo tome como una operación de algebra lineal}
        \PY{c+c1}{\PYZsh{} lo anterior es necesario con la función nn.Sequential()}
        \PY{n}{y} \PY{o}{=} \PY{n}{redDinamica}\PY{p}{(}\PY{n}{i}\PY{p}{)} 
        \PY{n}{yprima}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{autograd}\PY{o}{.}\PY{n}{grad}\PY{p}{(}\PY{n}{y}\PY{p}{,}\PY{n}{i}\PY{p}{,}\PY{n}{create\PYZus{}graph}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        \PY{n}{yprimaprima}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{autograd}\PY{o}{.}\PY{n}{grad}\PY{p}{(}\PY{n}{yprima}\PY{p}{,}\PY{n}{i}\PY{p}{,}\PY{n}{create\PYZus{}graph}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        \PY{n}{suma}\PY{o}{+}\PY{o}{=}\PY{p}{(}\PY{n}{yprimaprima}\PY{o}{+}\PY{n}{y}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
    \PY{c+c1}{\PYZsh{} Acá se usa alpha = 100}
    \PY{n}{suma}\PY{o}{+}\PY{o}{=}\PY{l+m+mi}{100}\PY{o}{*}\PY{p}{(}\PY{n}{redDinamica}\PY{p}{(}\PY{n}{x0}\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
    \PY{n}{suma}\PY{o}{+}\PY{o}{=}\PY{l+m+mi}{100}\PY{o}{*}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{redDinamica}\PY{p}{(}\PY{n}{xPi}\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
    \PY{n}{perdidavar} \PY{o}{=} \PY{n}{suma}
    \PY{k}{return} \PY{n}{suma}

\PY{k}{def} \PY{n+nf}{perdidaConPesos}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{k}{global} \PY{n}{perdidavar} \PY{c+c1}{\PYZsh{} esta variable guarda la perdida en una ubicación afuera de la función.}
    \PY{n}{x0} \PY{o}{=}  \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{p}{[}\PY{l+m+mf}{0.0}\PY{p}{]}\PY{p}{,}\PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{,}\PY{n}{requires\PYZus{}grad} \PY{o}{=}\PY{k+kc}{True}\PY{p}{)} \PY{c+c1}{\PYZsh{} Ubicación en X de la primera condición de frontera}
    \PY{c+c1}{\PYZsh{} el 0.0 es importante para que pytorch lo identifique como un float.}
    \PY{c+c1}{\PYZsh{} acá el argumento device se usa para indicar que el vector debe guardarse en la memoria de la gpu}
    \PY{c+c1}{\PYZsh{} Acá el rgumento requires\PYZus{}grad = true se usa para indicar que la variable importa a la hora de la diferenciación }
    \PY{n}{xPi} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mf}{3.14159}\PY{o}{/}\PY{l+m+mi}{2}\PY{p}{,}\PY{l+m+mi}{1}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{,}\PY{n}{device} \PY{o}{=} \PY{n}{device}\PY{p}{)} 
    \PY{n}{xPi} \PY{o}{=}  \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{p}{[}\PY{n}{xPi}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}\PY{p}{]}\PY{p}{,}\PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{,}\PY{n}{requires\PYZus{}grad} \PY{o}{=}\PY{k+kc}{True}\PY{p}{)} \PY{c+c1}{\PYZsh{} Ubicación en X se la segunda condición de frontera}
    \PY{n}{suma} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{n}{contador} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n}{puntos}\PY{p}{:}
        \PY{n}{i} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{,}\PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{,}\PY{n}{requires\PYZus{}grad} \PY{o}{=}\PY{k+kc}{True}\PY{p}{)} 
        \PY{c+c1}{\PYZsh{} hay que meter el valor en X dentro de un vector para que pytorch lo tome como una operación de algebra lineal}
        \PY{c+c1}{\PYZsh{} lo anterior es necesario con la función nn.Sequential()}
        \PY{n}{y} \PY{o}{=} \PY{n}{redDinamica}\PY{p}{(}\PY{n}{i}\PY{p}{)} 
        \PY{n}{yprima}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{autograd}\PY{o}{.}\PY{n}{grad}\PY{p}{(}\PY{n}{y}\PY{p}{,}\PY{n}{i}\PY{p}{,}\PY{n}{create\PYZus{}graph}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        \PY{n}{yprimaprima}\PY{o}{=}\PY{n}{torch}\PY{o}{.}\PY{n}{autograd}\PY{o}{.}\PY{n}{grad}\PY{p}{(}\PY{n}{yprima}\PY{p}{,}\PY{n}{i}\PY{p}{,}\PY{n}{create\PYZus{}graph}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{0}\PY{p}{]}
        \PY{n}{suma}\PY{o}{+}\PY{o}{=}\PY{n}{pesos}\PY{p}{[}\PY{n}{contador}\PY{p}{]}\PY{o}{*}\PY{p}{(}\PY{n}{yprimaprima}\PY{o}{+}\PY{n}{y}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
        \PY{n}{contador} \PY{o}{+}\PY{o}{=}\PY{l+m+mi}{1}
    \PY{c+c1}{\PYZsh{} Acá se usa alpha = 100}
    \PY{n}{suma}\PY{o}{+}\PY{o}{=}\PY{l+m+mi}{100}\PY{o}{*}\PY{p}{(}\PY{n}{redDinamica}\PY{p}{(}\PY{n}{x0}\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
    \PY{n}{suma}\PY{o}{+}\PY{o}{=}\PY{l+m+mi}{100}\PY{o}{*}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{\PYZhy{}}\PY{n}{redDinamica}\PY{p}{(}\PY{n}{xPi}\PY{p}{)}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}
    \PY{n}{perdidavar} \PY{o}{=} \PY{n}{suma}
    \PY{k}{return} \PY{n}{suma}

\PY{n}{perdidaConPesos}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{4}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
tensor([118.8364], device='cuda:0', grad\_fn=<AddBackward0>)
\end{Verbatim}
\end{tcolorbox}
        
    \hypertarget{condiciones-de-parada}{%
\subsection{Condiciones de parada}\label{condiciones-de-parada}}

Se van a usar dos condiciones de parada, la primera va a ser con
respecto al tiempo y la segunda va a ser con respecto a un criterio de
calidad

\hypertarget{tiempo}{%
\subsubsection{Tiempo}\label{tiempo}}

Para la condición de parada con respecto al tiempo se pondrá un limite
de 1.75 horas por caso de solución. Este criterio esta ahí porque el
computador con el cual se resuelve este problema no esta disponible de
forma ilimitada. Consecuentemente se desea que si se pasa el limite de
tiempo por lo menos se guarden las respuestas y avances.

\hypertarget{criterio-de-calidad}{%
\subsubsection{Criterio de calidad}\label{criterio-de-calidad}}

La segunda condición de parada va a ocurrir en caso de que la diferencia
promedio con respecto a la solución analítica sea de 0.001. Para evitar
evaluar dos veces la misma función se guarda el primer resultado y se
usa en el segundo

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{8}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Puntos Prueba : }
\PY{n}{puntosPrueba} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{7}\PY{p}{,}\PY{l+m+mi}{400}\PY{p}{,}\PY{n}{requires\PYZus{}grad}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}
\PY{n}{varPerdidaCondicionParada} \PY{o}{=} \PY{l+m+mf}{1e9}

\PY{k}{def} \PY{n+nf}{perdidaParaRevisar}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{k}{global} \PY{n}{varPerdidaCondicionParada}
    \PY{k}{with} \PY{n}{torch}\PY{o}{.}\PY{n}{no\PYZus{}grad}\PY{p}{(}\PY{p}{)}\PY{p}{:}
        \PY{n}{suma} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n}{puntosPrueba}\PY{p}{:}
            \PY{n}{i} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{,}\PY{n}{device}\PY{o}{=}\PY{n}{device}\PY{p}{)}
            \PY{n}{y} \PY{o}{=} \PY{n}{redDinamica}\PY{p}{(}\PY{n}{i}\PY{p}{)}
            \PY{n}{suma} \PY{o}{+}\PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{abs}\PY{p}{(}\PY{n}{y}\PY{o}{\PYZhy{}}\PY{n}{torch}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{n}{j}\PY{p}{)}\PY{p}{)}
    \PY{n}{varPerdidaCondicionParada} \PY{o}{=} \PY{n}{suma}
    \PY{k}{return} \PY{n}{suma}

\PY{k}{def} \PY{n+nf}{revisador}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{k}{return} \PY{n}{varPerdidaCondicionParada}\PY{o}{\PYZlt{}}\PY{l+m+mf}{0.01}\PY{o}{*}\PY{n+nb}{len}\PY{p}{(}\PY{n}{puntosPrueba}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{serializaciuxf3n-de-soluciones-y-anuxe1lisis-de-rata-de-aprendizaje.}{%
\subsection{Serialización de soluciones y análisis de rata de
aprendizaje.}\label{serializaciuxf3n-de-soluciones-y-anuxe1lisis-de-rata-de-aprendizaje.}}

La serialización es el proceso mediante el cual se guardan algunas
variables para que en análisis futuros se puedan usar sin necesitar
correr todo el código. \#\#\# subclases de pytorch Los objetos de
pytorch que se van a serializar vienen con un método que lo permite
hacer de forma sencilla. En este caso se usa este método
(save\_state\_dict()) \#\#\# Resto de objetos En este caso se usa la
libreria Pickle para este propósito. Para serializar primero se debe
abrir un archivo que contenga la información. Para esto se utiliza la
estructura try/except. esta va a intentar crear y abrir un archivo con
el nombre en filename, si esto falla intentara editar sobre un archivo
con ese nombre.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{9}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{if} \PY{n}{estaResolviendo}\PY{p}{:}
    \PY{n}{os}\PY{o}{.}\PY{n}{chdir}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{/home/externo/Documents/nico/efficientPINN/pythontesis}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{os}\PY{o}{.}\PY{n}{chdir}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{v5}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{filename} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RESULTADOS/registrosPerdidas/registro.tesis}\PY{l+s+s2}{\PYZdq{}}
\PY{k}{else}\PY{p}{:}
    \PY{n}{os}\PY{o}{.}\PY{n}{chdir}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{/home/externo/Documents/nico/efficientPINN/pythontesis}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{os}\PY{o}{.}\PY{n}{chdir}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{basura}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{filename} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RESULTADOS/registrosPerdidas/registro.tesis}\PY{l+s+s2}{\PYZdq{}}


\PY{k}{try}\PY{p}{:}
    \PY{n}{archivo} \PY{o}{=} \PY{n+nb}{open}\PY{p}{(}\PY{n}{filename}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{xb}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{k}{except}\PY{p}{:}
    \PY{n}{archivo} \PY{o}{=} \PY{n+nb}{open}\PY{p}{(}\PY{n}{filename}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{wb}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{anuxe1lisis-de-los-distintos-parametros-de-los-que-depende-el-algoritmo.}{%
\subsection{Análisis de los distintos parametros de los que depende el
algoritmo.}\label{anuxe1lisis-de-los-distintos-parametros-de-los-que-depende-el-algoritmo.}}

\hypertarget{orden}{%
\subsubsection{Orden}\label{orden}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Tasa de aprendizaje
\end{enumerate}

    \hypertarget{tasa-de-aprendizaje}{%
\subsubsection{Tasa de aprendizaje}\label{tasa-de-aprendizaje}}

En la siguiente celda se prueban 20 tasa de aprendizaje distribuidas
logarítmicamente entre \(10^{-2}\) y \(10^{-5}\). los resultados de esto
con el registro de las perdidas para cada tasa de aprendizaje quedan
serializados en el siguiente orden.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Separador llamado ``tasa de aprendizaje''
\item
  tasa de aprendizaje
\item
  registro de la evolución de la perdida con respecto al tiempo
\item
  Indicador de que termino o no termino
\item
  Tiempo en el que termino
\end{enumerate}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{10}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{if} \PY{n}{buscandoLearningRate}\PY{p}{:}
    \PY{k}{for} \PY{n}{learningRate} \PY{o+ow}{in} \PY{n+nb}{list}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{logspace}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{log10}\PY{p}{(}\PY{l+m+mf}{5e\PYZhy{}2}\PY{p}{)}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{log10}\PY{p}{(}\PY{l+m+mf}{1e\PYZhy{}6}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{18}\PY{p}{)}\PY{p}{)}\PY{p}{:}
        \PY{n}{pickle}\PY{o}{.}\PY{n}{dump}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{rata de aprendizaje}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{archivo}\PY{p}{)} \PY{c+c1}{\PYZsh{} Escribe un flag dentro del archivo de serialziación}
        \PY{n}{pickle}\PY{o}{.}\PY{n}{dump}\PY{p}{(}\PY{n}{learningRate}\PY{p}{,}\PY{n}{archivo}\PY{p}{)} \PY{c+c1}{\PYZsh{} Escribe la tasa de aprendizaje}
        \PY{n}{optimizer} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{redDinamica}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{n}{learningRate}\PY{p}{)} \PY{c+c1}{\PYZsh{} Se reinicializa el optimizador para cada ciclo}
        \PY{n}{registro\PYZus{}perdida}\PY{o}{=}\PY{p}{[}\PY{p}{]}
        \PY{n}{tiempoInicial} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
        \PY{n}{i} \PY{o}{=} \PY{l+m+mi}{0} \PY{c+c1}{\PYZsh{} i mantiene la cuenta de cuantas epocas se están resolviendo}
        \PY{n}{termino} \PY{o}{=} \PY{k+kc}{False}
        \PY{k}{while} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{n}{tiempoInicial}\PY{o}{\PYZlt{}}\PY{l+m+mi}{3600}\PY{o}{*}\PY{l+m+mf}{1.75} \PY{o+ow}{and} \PY{o+ow}{not} \PY{n}{termino} \PY{o+ow}{and} \PY{n}{estaResolviendo}\PY{p}{:}
            \PY{c+c1}{\PYZsh{} Compute prediction and loss}
            \PY{n}{loss} \PY{o}{=} \PY{n}{perdidaConPesos}\PY{p}{(}\PY{p}{)}
            \PY{c+c1}{\PYZsh{} Backpropagation}
            \PY{n}{optimizer}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}
            \PY{n}{loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
            \PY{n}{optimizer}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}

            \PY{k}{if} \PY{n}{i} \PY{o}{\PYZpc{}} \PY{l+m+mi}{10} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                \PY{c+c1}{\PYZsh{}print(loss.item()/len(puntos))}
                \PY{c+c1}{\PYZsh{}print(\PYZdq{}supermax\PYZdq{},promAct)}
                \PY{n}{actualizarPuntosConPesos}\PY{p}{(}\PY{p}{)}
            \PY{k}{if} \PY{n}{i} \PY{o}{\PYZpc{}} \PY{l+m+mi}{20} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                \PY{n}{registro\PYZus{}perdida}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{perdidaParaRevisar}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{puntos}\PY{p}{)}\PY{p}{)}
                \PY{n}{termino} \PY{o}{=} \PY{n}{revisador}\PY{p}{(}\PY{p}{)}
            \PY{k}{if} \PY{n}{i} \PY{o}{\PYZpc{}} \PY{l+m+mi}{300} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
                \PY{n}{torch}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{n}{redDinamica}\PY{o}{.}\PY{n}{state\PYZus{}dict}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RESULTADOS/estados/senoParalelo}\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{p}{)}\PY{o}{+}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ epochs}\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n+nb}{round}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{log10}\PY{p}{(}\PY{n}{learningRate}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}\PY{o}{+}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{.tar}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
            \PY{n}{i}\PY{o}{+}\PY{o}{=}\PY{l+m+mi}{1}
        \PY{n}{pickle}\PY{o}{.}\PY{n}{dump}\PY{p}{(}\PY{n}{registro\PYZus{}perdida}\PY{p}{,}\PY{n}{archivo}\PY{p}{)}
        \PY{k}{if} \PY{n}{termino}\PY{p}{:}
            \PY{n}{pickle}\PY{o}{.}\PY{n}{dump}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{termino en}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{archivo}\PY{p}{)}
            \PY{n}{pickle}\PY{o}{.}\PY{n}{dump}\PY{p}{(}\PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{n}{tiempoInicial}\PY{p}{,}\PY{n}{archivo}\PY{p}{)}
        \PY{n}{torch}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{n}{redDinamica}\PY{o}{.}\PY{n}{state\PYZus{}dict}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RESULTADOS/terminados/senoParaleloFinales}\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{log10}\PY{p}{(}\PY{n}{learningRate}\PY{p}{)}\PY{p}{)}\PY{o}{+}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{.tar}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{learningRate}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{i}\PY{p}{)}
        \PY{n}{redDinamica} \PY{o}{=} \PY{n}{NeuralNetworkPrueba}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
\PY{n}{archivo}\PY{o}{.}\PY{n}{close}\PY{p}{(}\PY{p}{)}
\PY{n}{c1} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}5}
\PY{n}{c2} \PY{o}{=} \PY{l+m+mi}{1}\PY{c+c1}{\PYZsh{}\PYZsh{} RESULTADOS DE ANALISIS DE RATA DE APRENDIZAJE.}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{resultados-de-analisis-de-rata-de-aprendizaje.}{%
\subsection{RESULTADOS DE ANALISIS DE RATA DE
APRENDIZAJE.}\label{resultados-de-analisis-de-rata-de-aprendizaje.}}

\hypertarget{tiempo-de-convergencia-vs-rata-de-aprendizaje.}{%
\subsubsection{Tiempo de convergencia vs rata de
aprendizaje.}\label{tiempo-de-convergencia-vs-rata-de-aprendizaje.}}

Los primeros resultados de la rata de convergencia indican que el
resultado ideal es alrededor de 1e-3. Estos se pueden ver en la gráfica
de la siguiente celda. Los valores probados fueron sugeridos por esta
fuente Esto es un valor convencional para el optimizador ADAM.
Adicionalmente los resultados se ven similares a los de esta fuentes .
Vale la pena recapitular que las estrellas verdes corresponden a los
intentos que no cumplieron el objetivo de calidad en 1.75 horas.

\hypertarget{forma-de-los-residuales-para-distintos-grupos-de-rata-de-aprendizaje.}{%
\subsubsection{Forma de los residuales para distintos grupos de rata de
aprendizaje.}\label{forma-de-los-residuales-para-distintos-grupos-de-rata-de-aprendizaje.}}

La gráfica de la forma de los residuales para distintos grupos de rata
de aprendizaje muestra como se comportaron los residuales o valores de
la función de perdida a través del proceso. Las líneas azules
corresponden a las ratas de aprendizaje demasiado agresivas. Acá se
puede ver como inicialmente logran disminuir la función de perdida pero
rápidamente se estancan. Por otro lado las líneas rosadas corresponden a
las ratas de aprendizaje demasiado lentas. Acá las líneas nunca dejan de
progresar, pero tampoco se acercan a terminar. Finalmente, las líneas
amarillas corresponden a los casos en los que se termina antes de 1.75
horas. Hay que notar que para este problema en especifico, es esperable
que el solucionador acabe en menos de 5 000 épocas.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{11}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{convergencia1}\PY{p}{(}\PY{p}{)}\PY{p}{:} \PY{c+c1}{\PYZsh{}comentario}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,}\PY{n}{archivo}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{rata}\PY{p}{:} \PY{n+nb}{float} \PY{o}{=} \PY{l+m+mi}{0}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{tiempo}\PY{p}{:} \PY{n+nb}{float} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{perdidas} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{generador}\PY{p}{(}\PY{n}{archivo}\PY{p}{)}
    \PY{k}{def} \PY{n+nf}{lectura}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,}\PY{n}{archivo}\PY{p}{)}\PY{p}{:}
        \PY{k}{try}\PY{p}{:}
            \PY{n}{leido} \PY{o}{=} \PY{n}{pickle}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{n}{archivo}\PY{p}{)}
        \PY{k}{except} \PY{n+ne}{Exception} \PY{k}{as} \PY{n}{e}\PY{p}{:}
            \PY{k}{if} \PY{n}{e}\PY{o}{.}\PY{n}{args}\PY{o}{==}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Ran out of input}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{p}{)}\PY{p}{:}
                \PY{k}{raise} \PY{n+ne}{Exception}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ya no hay nada mas que leer indicado por pickle}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{k}{return} \PY{n}{leido}
    \PY{k}{def} \PY{n+nf}{generador}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,}\PY{n}{archivo}\PY{p}{)}\PY{p}{:}
        \PY{k}{global} \PY{n}{lecturaDeTiempo}
        \PY{k}{if} \PY{o+ow}{not} \PY{n}{lecturaDeTiempo}\PY{p}{:}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{rata} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{lectura}\PY{p}{(}\PY{n}{archivo}\PY{p}{)}
        \PY{k}{elif} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{lectura}\PY{p}{(}\PY{n}{archivo}\PY{p}{)} \PY{o}{==} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{rata de aprendizaje}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{rata} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{lectura}\PY{p}{(}\PY{n}{archivo}\PY{p}{)}
        \PY{k}{else}\PY{p}{:}
            \PY{k}{raise} \PY{n+ne}{Exception}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Se putio el orden de lectura}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{perdidas} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{lectura}\PY{p}{(}\PY{n}{archivo}\PY{p}{)}
        \PY{k}{if} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{lectura}\PY{p}{(}\PY{n}{archivo}\PY{p}{)} \PY{o}{==} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{termino en}\PY{l+s+s2}{\PYZdq{}}\PY{p}{:}
            \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{tiempo} \PY{o}{=} \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{lectura}\PY{p}{(}\PY{n}{archivo}\PY{p}{)}
            \PY{n}{lecturaDeTiempo} \PY{o}{=} \PY{k+kc}{True}
        \PY{k}{else}\PY{p}{:}
            \PY{n}{lecturaDeTiempo} \PY{o}{=} \PY{k+kc}{False}

\PY{n}{convergencias1} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{c+c1}{\PYZsh{} importada de archivos pasados}
\PY{n}{os}\PY{o}{.}\PY{n}{chdir}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{/home/externo/Documents/nico/efficientPINN/pythontesis}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{os}\PY{o}{.}\PY{n}{chdir}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{v4/RESULTADOS/registrosPerdidas}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{lecturaDeTiempo} \PY{o}{=} \PY{k+kc}{True}
\PY{n}{archivo} \PY{o}{=} \PY{n+nb}{open}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{registro.tesis}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{rb}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{k}{while} \PY{k+kc}{True}\PY{p}{:}
    \PY{k}{try}\PY{p}{:}
        \PY{n}{objetoTemporal} \PY{o}{=} \PY{n}{convergencia1}\PY{p}{(}\PY{n}{archivo}\PY{p}{)}
        \PY{n}{convergencias1}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{objetoTemporal}\PY{p}{)}
    \PY{k}{except} \PY{n+ne}{Exception} \PY{k}{as} \PY{n}{e}\PY{p}{:}
        \PY{n+nb}{print} \PY{p}{(}\PY{n}{e}\PY{o}{.}\PY{n}{args}\PY{p}{)}
        \PY{n+nb}{print} \PY{p}{(}\PY{n}{e}\PY{o}{.}\PY{n}{\PYZus{}\PYZus{}traceback\PYZus{}\PYZus{}}\PY{p}{)}
        \PY{k}{break}
\PY{n}{archivo}\PY{o}{.}\PY{n}{close}\PY{p}{(}\PY{p}{)}
\PY{n}{lecturaDeTiempo} \PY{o}{=} \PY{k+kc}{True}
\PY{n}{archivo} \PY{o}{=} \PY{n+nb}{open}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{registro2.tesis}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{rb}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{k}{while} \PY{k+kc}{True}\PY{p}{:}
    \PY{k}{try}\PY{p}{:}
        \PY{n}{objetoTemporal} \PY{o}{=} \PY{n}{convergencia1}\PY{p}{(}\PY{n}{archivo}\PY{p}{)}
        \PY{n}{convergencias1}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{objetoTemporal}\PY{p}{)}
    \PY{k}{except} \PY{n+ne}{Exception} \PY{k}{as} \PY{n}{e}\PY{p}{:}
        \PY{n+nb}{print} \PY{p}{(}\PY{n}{e}\PY{o}{.}\PY{n}{args}\PY{p}{)}
        \PY{n+nb}{print} \PY{p}{(}\PY{n}{e}\PY{o}{.}\PY{n}{\PYZus{}\PYZus{}traceback\PYZus{}\PYZus{}}\PY{p}{)}
        \PY{k}{break}
\PY{n}{archivo}\PY{o}{.}\PY{n}{close}\PY{p}{(}\PY{p}{)}
\PY{n}{os}\PY{o}{.}\PY{n}{chdir}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{/home/externo/Documents/nico/efficientPINN/pythontesis}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{os}\PY{o}{.}\PY{n}{chdir}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{v5/RESULTADOS/registrosPerdidas}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{lecturaDeTiempo} \PY{o}{=} \PY{k+kc}{True}
\PY{n}{archivo} \PY{o}{=} \PY{n+nb}{open}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{registro.tesis}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{rb}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{k}{while} \PY{k+kc}{True}\PY{p}{:}
    \PY{k}{try}\PY{p}{:}
        \PY{n}{objetoTemporal} \PY{o}{=} \PY{n}{convergencia1}\PY{p}{(}\PY{n}{archivo}\PY{p}{)}
        \PY{n}{convergencias1}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{objetoTemporal}\PY{p}{)}
    \PY{k}{except} \PY{n+ne}{Exception} \PY{k}{as} \PY{n}{e}\PY{p}{:}
        \PY{n+nb}{print} \PY{p}{(}\PY{n}{e}\PY{o}{.}\PY{n}{args}\PY{p}{)}
        \PY{n+nb}{print} \PY{p}{(}\PY{n}{e}\PY{o}{.}\PY{n}{\PYZus{}\PYZus{}traceback\PYZus{}\PYZus{}}\PY{p}{)}
        \PY{k}{break}
\PY{n}{archivo}\PY{o}{.}\PY{n}{close}\PY{p}{(}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{convergencias1}\PY{p}{)}\PY{p}{)}

\PY{n}{convergencias1} \PY{o}{=} \PY{n+nb}{sorted}\PY{p}{(}\PY{n}{convergencias1}\PY{p}{,}\PY{n}{key} \PY{o}{=} \PY{k}{lambda} \PY{n}{x}\PY{p}{:}\PY{n}{x}\PY{o}{.}\PY{n}{rata}\PY{p}{)}

\PY{n}{ratasDeConvergencia1} \PY{o}{=} \PY{p}{[}\PY{n}{simulacion}\PY{o}{.}\PY{n}{rata} \PY{k}{for} \PY{n}{simulacion} \PY{o+ow}{in} \PY{n}{convergencias1} \PY{k}{if} \PY{n}{simulacion}\PY{o}{.}\PY{n}{tiempo} \PY{o}{!=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
\PY{n}{tiemposDeConvergencia1} \PY{o}{=} \PY{p}{[}\PY{n}{simulacion}\PY{o}{.}\PY{n}{tiempo} \PY{k}{for} \PY{n}{simulacion} \PY{o+ow}{in} \PY{n}{convergencias1} \PY{k}{if} \PY{n}{simulacion}\PY{o}{.}\PY{n}{tiempo} \PY{o}{!=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}

\PY{n}{ratasDeConvergenciasFallidas} \PY{o}{=} \PY{p}{[}\PY{n}{simulacion}\PY{o}{.}\PY{n}{rata} \PY{k}{for} \PY{n}{simulacion} \PY{o+ow}{in} \PY{n}{convergencias1} \PY{k}{if} \PY{n}{simulacion}\PY{o}{.}\PY{n}{tiempo} \PY{o}{==} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
\PY{n}{yTemporal} \PY{o}{=} \PY{p}{[}\PY{n+nb}{max}\PY{p}{(}\PY{n}{tiemposDeConvergencia1}\PY{p}{)}\PY{p}{]}\PY{o}{*}\PY{n+nb}{len}\PY{p}{(}\PY{n}{ratasDeConvergenciasFallidas}\PY{p}{)}


\PY{n}{eleccion} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argpartition}\PY{p}{(}\PY{n}{tiemposDeConvergencia1}\PY{p}{,}\PY{l+m+mi}{5}\PY{p}{)}
\PY{n}{eleccion} \PY{o}{=} \PY{n}{eleccion}\PY{p}{[}\PY{l+m+mi}{1}\PY{p}{]}
\PY{n}{os}\PY{o}{.}\PY{n}{chdir}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{/home/externo/Documents/nico/efficientPINN/pythontesis}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{os}\PY{o}{.}\PY{n}{chdir}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{graficas2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize}\PY{o}{=}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{l+m+mi}{6}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{loglog}\PY{p}{(}\PY{n}{ratasDeConvergencia1}\PY{p}{,}\PY{n}{tiemposDeConvergencia1}\PY{p}{,}\PY{n}{marker} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{o}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{ms} \PY{o}{=} \PY{l+m+mi}{6}\PY{p}{,}\PY{n}{linewidth} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Tiempos tomados para la convergencia}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{loglog}\PY{p}{(}\PY{n}{ratasDeConvergencia1}\PY{p}{[}\PY{n}{eleccion}\PY{p}{]}\PY{p}{,}\PY{n}{tiemposDeConvergencia1}\PY{p}{[}\PY{n}{eleccion}\PY{p}{]}\PY{p}{,}\PY{n}{marker} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{+}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{ms}\PY{o}{=}\PY{l+m+mi}{15}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{tiempo Elegido para el resto de simulaciones}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{linewidth} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{loglog}\PY{p}{(}\PY{n}{ratasDeConvergenciasFallidas}\PY{p}{,}\PY{n}{yTemporal}\PY{p}{,}\PY{n}{marker} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{*}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{ms}\PY{o}{=}\PY{l+m+mi}{10}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Convergencia fallida}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{linewidth} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Resultados del analisis de tasa de aprendizaje}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{k+kc}{True}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{both}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{n}{bbox\PYZus{}to\PYZus{}anchor} \PY{o}{=} \PY{p}{(}\PY{l+m+mf}{0.8}\PY{p}{,}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.1}\PY{p}{)}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Tiempo tomado para la convergencia}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Rata de aprendizaje}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{savefig}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{tiempoDeConvergenciaVsLearningRate.png}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{dpi} \PY{o}{=} \PY{l+m+mi}{300}\PY{p}{,} \PY{n}{bbox\PYZus{}inches} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{tight}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\PY{k+kn}{import} \PY{n+nn}{scipy}\PY{n+nn}{.}\PY{n+nn}{signal} \PY{k}{as} \PY{n+nn}{signal}
\PY{n}{convergieron} \PY{o}{=} \PY{p}{[}\PY{n}{i}\PY{o}{.}\PY{n}{perdidas} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{convergencias1} \PY{k}{if} \PY{n}{i}\PY{o}{.}\PY{n}{tiempo} \PY{o}{!=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
\PY{n}{ratasTemporales} \PY{o}{=} \PY{p}{[}\PY{n}{i}\PY{o}{.}\PY{n}{rata} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{convergencias1} \PY{k}{if} \PY{n}{i}\PY{o}{.}\PY{n}{tiempo} \PY{o}{!=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
\PY{n}{noConvergieronArriba} \PY{o}{=} \PY{p}{[}\PY{n}{i}\PY{o}{.}\PY{n}{perdidas} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{convergencias1} \PY{k}{if} \PY{n}{i}\PY{o}{.}\PY{n}{tiempo} \PY{o}{==} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1} \PY{o+ow}{and} \PY{n}{i}\PY{o}{.}\PY{n}{rata} \PY{o}{\PYZgt{}} \PY{l+m+mf}{1e\PYZhy{}3}\PY{p}{]}
\PY{n}{noConvergieronAbajo} \PY{o}{=} \PY{p}{[}\PY{n}{i}\PY{o}{.}\PY{n}{perdidas} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{convergencias1} \PY{k}{if} \PY{n}{i}\PY{o}{.}\PY{n}{tiempo} \PY{o}{==} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1} \PY{o+ow}{and} \PY{n}{i}\PY{o}{.}\PY{n}{rata} \PY{o}{\PYZlt{}} \PY{l+m+mf}{1e\PYZhy{}3}\PY{p}{]}

\PY{k}{def} \PY{n+nf}{filtrador}\PY{p}{(}\PY{n}{x\PYZus{}}\PY{p}{)}\PY{p}{:}
    \PY{n}{b}\PY{p}{,} \PY{n}{a} \PY{o}{=} \PY{n}{signal}\PY{o}{.}\PY{n}{butter}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{,} \PY{l+m+mf}{0.075}\PY{p}{,} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{low}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{k}{return} \PY{n}{signal}\PY{o}{.}\PY{n}{filtfilt}\PY{p}{(}\PY{n}{b}\PY{p}{,} \PY{n}{a}\PY{p}{,} \PY{n}{x\PYZus{}}\PY{p}{)}

\PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{figsize} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,}\PY{l+m+mi}{9}\PY{p}{)}\PY{p}{)}


\PY{n}{os}\PY{o}{.}\PY{n}{chdir}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{/home/externo/Documents/nico/efficientPINN/pythontesis}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{os}\PY{o}{.}\PY{n}{chdir}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{graficas2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title} \PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Velocidad de convergencia vs epocas para distintas ratas de aprendizaje}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}


\PY{n}{transparencia} \PY{o}{=} \PY{l+m+mi}{1}

\PY{n}{mejorOpcion} \PY{o}{=} \PY{l+m+mi}{0}
\PY{k}{for} \PY{n}{j} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{convergieron}\PY{p}{)}\PY{p}{)}\PY{p}{:}
    \PY{n}{i} \PY{o}{=} \PY{n}{convergieron}\PY{p}{[}\PY{n}{j}\PY{p}{]}
    \PY{k}{if} \PY{n}{convergencias1}\PY{p}{[}\PY{n}{eleccion}\PY{p}{]}\PY{o}{.}\PY{n}{rata} \PY{o}{==} \PY{n}{ratasTemporales}\PY{p}{[}\PY{n}{j}\PY{p}{]}\PY{p}{:}
        \PY{n}{mejorOpcion} \PY{o}{=} \PY{n}{j}
    \PY{k}{else}\PY{p}{:}
        \PY{n}{senal\PYZus{}filtrada} \PY{o}{=} \PY{n}{filtrador}\PY{p}{(}\PY{n}{i}\PY{p}{)}
        \PY{n}{eje\PYZus{}x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{senal\PYZus{}filtrada}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{20}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{senal\PYZus{}filtrada}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} Se tomo un sample cada 300 epocas.  }
        \PY{n}{ax}\PY{o}{.}\PY{n}{semilogy}\PY{p}{(}\PY{n}{eje\PYZus{}x}\PY{p}{,}\PY{n}{senal\PYZus{}filtrada}\PY{p}{,}\PY{n}{color} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}F4B942}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Rata de aprendizaje adecuada}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{alpha} \PY{o}{=} \PY{n}{transparencia}\PY{p}{)}

\PY{n}{senal\PYZus{}filtrada} \PY{o}{=}  \PY{n}{filtrador}\PY{p}{(}\PY{n}{convergieron}\PY{p}{[}\PY{n}{mejorOpcion}\PY{p}{]}\PY{p}{,}\PY{p}{)}
\PY{n}{eje\PYZus{}x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{senal\PYZus{}filtrada}\PY{p}{)} \PY{o}{*} \PY{l+m+mi}{20}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{senal\PYZus{}filtrada}\PY{p}{)}\PY{p}{)} 
\PY{n}{ax}\PY{o}{.}\PY{n}{semilogy}\PY{p}{(}\PY{n}{eje\PYZus{}x}\PY{p}{,}\PY{n}{senal\PYZus{}filtrada}\PY{p}{,}\PY{n}{color} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}4059AD}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Rata de aprendizaje elegida}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{alpha} \PY{o}{=} \PY{l+m+mi}{1}\PY{p}{,}\PY{n}{linewidth} \PY{o}{=} \PY{l+m+mf}{1.5}\PY{p}{,}\PY{n}{linestyle} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}      

\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{noConvergieronArriba}\PY{p}{:}
    \PY{n}{senal\PYZus{}filtrada} \PY{o}{=} \PY{n}{filtrador}\PY{p}{(}\PY{n}{i}\PY{p}{)}
    \PY{n}{eje\PYZus{}x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{senal\PYZus{}filtrada}\PY{p}{)}\PY{o}{*} \PY{l+m+mi}{20}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{senal\PYZus{}filtrada}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} Se tomo un sample cada 300 epocas.  }
    \PY{n}{ax}\PY{o}{.}\PY{n}{semilogy}\PY{p}{(}\PY{n}{eje\PYZus{}x}\PY{p}{,}\PY{n}{senal\PYZus{}filtrada}\PY{p}{,}\PY{n}{color} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}6B9AC4}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Rata de aprendizaje Demasiado Agresiva}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{alpha} \PY{o}{=} \PY{n}{transparencia}\PY{p}{)}

\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{noConvergieronAbajo}\PY{p}{:}
    \PY{n}{senal\PYZus{}filtrada} \PY{o}{=} \PY{n}{filtrador}\PY{p}{(}\PY{n}{i}\PY{p}{)}
    \PY{n}{eje\PYZus{}x} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{senal\PYZus{}filtrada}\PY{p}{)}\PY{o}{*}\PY{l+m+mi}{20}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{senal\PYZus{}filtrada}\PY{p}{)}\PY{p}{)} \PY{c+c1}{\PYZsh{} Se tomo un sample cada 300 epocas.  }
    \PY{n}{ax}\PY{o}{.}\PY{n}{semilogy}\PY{p}{(}\PY{n}{eje\PYZus{}x}\PY{p}{,}\PY{n}{senal\PYZus{}filtrada}\PY{p}{,}\PY{n}{color} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZsh{}FC94AD}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Rata de aprendizaje Demasiado Lenta}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{alpha} \PY{o}{=} \PY{n}{transparencia}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Residuales}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{epocas}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{n}{which}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{both}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{display} \PY{o}{=} \PY{p}{[}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{15}\PY{p}{,}\PY{l+m+mi}{24}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{convergieron}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
\PY{n}{handles}\PY{p}{,} \PY{n}{labels} \PY{o}{=} \PY{n}{ax}\PY{o}{.}\PY{n}{get\PYZus{}legend\PYZus{}handles\PYZus{}labels}\PY{p}{(}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{[}\PY{n}{handle} \PY{k}{for} \PY{n}{i}\PY{p}{,}\PY{n}{handle} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{handles}\PY{p}{)} \PY{k}{if} \PY{n}{i} \PY{o+ow}{in} \PY{n}{display}\PY{p}{]}\PY{p}{,}
      \PY{p}{[}\PY{n}{label} \PY{k}{for} \PY{n}{i}\PY{p}{,}\PY{n}{label} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{labels}\PY{p}{)} \PY{k}{if} \PY{n}{i} \PY{o+ow}{in} \PY{n}{display}\PY{p}{]}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{style}\PY{o}{.}\PY{n}{context}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{seaborn\PYZhy{}colorblind}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{savefig}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Convergencia para distintas ratas de aprendizaje}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{dpi} \PY{o}{=} \PY{l+m+mi}{300}\PY{p}{,} \PY{n}{bbox\PYZus{}inches} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{tight}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
('ya no hay nada mas que leer indicado por pickle',)
<traceback object at 0x7ff900c3bdc0>
('ya no hay nada mas que leer indicado por pickle',)
<traceback object at 0x7ff900c5aa40>
('ya no hay nada mas que leer indicado por pickle',)
<traceback object at 0x7ff900c581c0>
25
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Final_files/Final_17_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Final_files/Final_17_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{analisis-de-la-soluciuxf3n-de-uno-de-los-casos-anteriores.}{%
\subsubsection{Analisis de la solución de uno de los casos
anteriores.}\label{analisis-de-la-soluciuxf3n-de-uno-de-los-casos-anteriores.}}

la siguiente celda no muestra resultados porque no hay una gpu
disponible para que los muestre. Intentar correrla en cpu mata el kernel

Como se puede ver en la solución de la celda a continuación,
inicialmente se converje a la respuesta desde la parte del dominio
cercana a las condiciones de frontera y luego esta solución se propaga
al resto del dominio.

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{33}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{os}\PY{o}{.}\PY{n}{chdir}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{/home/externo/Documents/nico/efficientPINN/pythontesis}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{os}\PY{o}{.}\PY{n}{chdir}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{v4/RESULTADOS/estados}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{archivos} \PY{o}{=} \PY{p}{[}\PY{n}{archivito} \PY{k}{for} \PY{n}{archivito} \PY{o+ow}{in} \PY{n}{os}\PY{o}{.}\PY{n}{listdir}\PY{p}{(}\PY{p}{)} \PY{k}{if} \PY{n}{archivito}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{8}\PY{p}{:}\PY{p}{]}\PY{o}{==}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{4.66.tar}\PY{l+s+s2}{\PYZdq{}}  \PY{p}{]}
\PY{n+nb}{print}\PY{p}{(}\PY{n}{archivos}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{figure}\PY{p}{(}\PY{n}{figsize} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{20}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{)}\PY{p}{)}
\PY{n}{alpha} \PY{o}{=} \PY{l+m+mi}{0}
\PY{k}{for} \PY{n}{nombre} \PY{o+ow}{in} \PY{n}{archivos}\PY{p}{:}
    \PY{n}{redDinamica}\PY{o}{.}\PY{n}{load\PYZus{}state\PYZus{}dict}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{n}{nombre}\PY{p}{)}\PY{p}{)}
    \PY{n}{ygrafica} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{n}{puntosGrafica} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{250}\PY{p}{)}
    \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{puntosGrafica}\PY{p}{:}
        \PY{n}{ytemp}\PY{o}{=}\PY{n}{redDinamica}\PY{p}{(}\PY{n}{torch}\PY{o}{.}\PY{n}{tensor}\PY{p}{(}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,}\PY{n}{device} \PY{o}{=} \PY{n}{device}\PY{p}{)}\PY{p}{)}
        \PY{n}{ygrafica}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{ytemp}\PY{o}{.}\PY{n}{detach}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{p}{)}
        \PY{c+c1}{\PYZsh{}ygrafica.append(ytemp.detach().numpy()[0])}
    \PY{k+kn}{import} \PY{n+nn}{numpy} \PY{k}{as} \PY{n+nn}{np}
    \PY{n}{puntosGrafica} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{l+m+mi}{0}\PY{p}{,}\PY{l+m+mi}{10}\PY{p}{,}\PY{l+m+mi}{250}\PY{p}{)}
    \PY{k}{if} \PY{n}{nombre} \PY{o}{!=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{senoParalelo0 epochs\PYZhy{}4.66.tar}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
        \PY{n}{epoca} \PY{o}{=} \PY{n+nb}{int}\PY{p}{(}\PY{n}{nombre}\PY{p}{[}\PY{l+m+mi}{12}\PY{p}{:}\PY{l+m+mi}{16}\PY{p}{]}\PY{p}{)}
    \PY{k}{else}\PY{p}{:}
        \PY{n}{epoca} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{n}{alpha} \PY{o}{=} \PY{n}{epoca}\PY{o}{/}\PY{l+m+mf}{1e5}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{puntosGrafica}\PY{p}{,}\PY{n}{ygrafica}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{n}{epoca}\PY{p}{,}\PY{n}{alpha} \PY{o}{=} \PY{n}{alpha}\PY{p}{)}
    
\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{puntosGrafica}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{sin}\PY{p}{(}\PY{n}{puntosGrafica}\PY{p}{)}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{referencia}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{linestyle}\PY{o}{=}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZhy{}.}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
['senoParalelo5100 epochs-4.66.tar', 'senoParalelo2100 epochs-4.66.tar',
'senoParalelo2700 epochs-4.66.tar', 'senoParalelo6900 epochs-4.66.tar',
'senoParalelo300 epochs-4.66.tar', 'senoParalelo8700 epochs-4.66.tar',
'senoParalelo0 epochs-4.66.tar', 'senoParalelo3900 epochs-4.66.tar',
'senoParalelo10800 epochs-4.66.tar', 'senoParalelo11100 epochs-4.66.tar',
'senoParalelo900 epochs-4.66.tar', 'senoParalelo1500 epochs-4.66.tar',
'senoParalelo4200 epochs-4.66.tar', 'senoParalelo9900 epochs-4.66.tar',
'senoParalelo10200 epochs-4.66.tar', 'senoParalelo6000 epochs-4.66.tar',
'senoParalelo9300 epochs-4.66.tar', 'senoParalelo7200 epochs-4.66.tar',
'senoParalelo6300 epochs-4.66.tar', 'senoParalelo8400 epochs-4.66.tar',
'senoParalelo3000 epochs-4.66.tar', 'senoParalelo600 epochs-4.66.tar',
'senoParalelo1800 epochs-4.66.tar', 'senoParalelo5700 epochs-4.66.tar',
'senoParalelo3600 epochs-4.66.tar', 'senoParalelo2400 epochs-4.66.tar',
'senoParalelo5400 epochs-4.66.tar', 'senoParalelo9000 epochs-4.66.tar',
'senoParalelo4800 epochs-4.66.tar', 'senoParalelo6600 epochs-4.66.tar',
'senoParalelo9600 epochs-4.66.tar', 'senoParalelo3300 epochs-4.66.tar',
'senoParalelo4500 epochs-4.66.tar', 'senoParalelo7800 epochs-4.66.tar',
'senoParalelo1200 epochs-4.66.tar', 'senoParalelo10500 epochs-4.66.tar',
'senoParalelo7500 epochs-4.66.tar', 'senoParalelo8100 epochs-4.66.tar']
    \end{Verbatim}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{33}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<matplotlib.legend.Legend at 0x7fc0c0263af0>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Final_files/Final_19_2.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{proceso-de-optimizaciuxf3n}{%
\subsubsection{Proceso de
optimización}\label{proceso-de-optimizaciuxf3n}}

Se va a optimizar para encontrar como el tiempo que toma llegar a una
solución es afectado por las siguientes variables. Dado que el tiempo
que toma llegar a una solución es una variable no determinística, no se
considera que sea apropiado usar métodos que utilicen gradiente para
esta optimización. Esto se debe a que las gradientes que se van a
encontrar van a estar fuertemente influenciadas por el ruido. Para
remediar esto se va a optimizar a partir de interpolaciones con procesos
iterativos de polinomios de chebyscheff. Esto se va a explicar a
continuación.

\begin{itemize}
\tightlist
\item
  Rata de aprendizaje de ADAM
\item
  \(\sigma\)
\item
  \(c_1\)
\item
  \(c_2\)
\end{itemize}

\hypertarget{punto-de-chebyscheff}{%
\subsubsection{Punto de Chebyscheff}\label{punto-de-chebyscheff}}

Para el proceso de interpolación se utilizará el método de interpolación
de LaGrange. Este método genera una un polinomio \(p_n(x)\) de grado n
cuyo valor es igual a una función \(f(x)\) en al menos n puntos. Los
puntos en los cuales son iguales se pueden elegir, deben ser diferentes
entre sí y están representados en el siguiente conjunto
\{\(x_0 , ... , x_n\)\}. Dado que la interpolación no necesariamente es
exacta para todos los puntos dentro del dominio, puede existir un error
para ciertos puntos del dominio. La elección de los puntos afecta
significativamente la distribución del error dentro de la función. A
continuación se describe la formula asociada al error numérico por
aproximación polinómica y la razón por la cual se eligen los puntos de
chebyscheff.

\[f(x) - p_n(x) = \frac{f^{(n + 1)}(\xi)}{(n+1)!} \blue {\prod_{i=0}^n{x-x_i}} \]

En la formula anterior \(\xi\) es un punto dentro del intervalo de
interpolación. Actualmente no se conoce cual punto es ni como asignarlo
a algún punto deseado. Consecuentemente la única parte controlable es la
que esta resaltada en azul. Los puntos que minimizan el valor de la
multiplicatoria son los puntos de chebyscheff. La comprobación de esto
se puede encontrar en está página . Dado que se esta minimizando la
parte controlable del error, se considera que se está minimizando el
error. la formula para los puntos \({x_0,..., x_n}\) se encuentra a
continuación.

Para cada punto \(x_i\) en el intervalo {[}-1,1{]}

\[x_i = cos (\frac{\pi (k + 0.5)}{n}) \text{ para } k = 0,...,n \]

Para pasar al intervalo {[}a,b{]} se introduce la transformación
\(t(x) = \frac{a + b + (b-a)x}{1}\) que lleva a la siguiente expresión.

\[x_i = \frac{a + b + (b-a)*cos (\frac{\pi (k + 0.5)}{n})}{2} \text{ para } k = 0,...,n \]

    \hypertarget{implementaciuxf3n-de-los-puntos-de-chebyscheff-y-optimizaciuxf3n-de-parametros}{%
\subsection{Implementación de los puntos de chebyscheff y optimización
de
parametros}\label{implementaciuxf3n-de-los-puntos-de-chebyscheff-y-optimizaciuxf3n-de-parametros}}

La implementación del algoritmo de optimización propuesto se hace de la
siguiente forma 1. Se define la función que da los puntos de chebyscheff
2. Se Prueba el algoritmo para encontrar el mínimo de la función
\(f(x) = - \frac{1}{1-(tan(x)-1)^2}\) 3. Se implementa el algoritmo para
encontrar el parámetro \(c_2\) que menos tiempo toma para converger 4.
Se implementa el algoritmo para encontrar el parámetro \(c_1\) que menos
tiempo toma para converger 5. Se implementa el algoritmo para encontrar
el parámetro \(\sigma\) que menos tiempo toma para converger. 6. Se
implementa el paso 3, 4 y 5 dos veces más para ya que \(\sigma\),
\(c_1\) y \(c_2\) deberían depender entre si.

    \hypertarget{implementaciuxf3n-de-la-funciuxf3n-de-puntos-y-prueba-del-algoritmo}{%
\subsubsection{Implementación de la función de puntos y Prueba del
algoritmo}\label{implementaciuxf3n-de-la-funciuxf3n-de-puntos-y-prueba-del-algoritmo}}

En la implementación a continuación se puede ver como el proceso de
optimización funciona para el ejemplo dado

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{12}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{def} \PY{n+nf}{puntos\PYZus{}chebyscheff}\PY{p}{(}\PY{n}{n}\PY{p}{,}\PY{n}{a}\PY{p}{,}\PY{n}{b}\PY{p}{)}\PY{p}{:}
    \PY{n}{inicializacion} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n+nb}{range}\PY{p}{(}\PY{n}{n}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{)}
    \PY{n}{inicializacion} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{flip}\PY{p}{(}\PY{n}{inicializacion}\PY{p}{)}
    \PY{n}{inicializacion} \PY{o}{=} \PY{n}{inicializacion}\PY{p}{[}\PY{p}{:}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]}
    \PY{k}{return} \PY{p}{(}\PY{n}{a} \PY{o}{+}\PY{n}{b} \PY{o}{+} \PY{p}{(}\PY{n}{b}\PY{o}{\PYZhy{}}\PY{n}{a}\PY{p}{)}\PY{o}{*}\PY{n}{np}\PY{o}{.}\PY{n}{cos}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{pi}\PY{o}{/}\PY{n}{n}\PY{o}{*}\PY{p}{(}\PY{n}{inicializacion} \PY{o}{\PYZhy{}} \PY{l+m+mf}{0.5}\PY{p}{)}\PY{p}{)}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{2}

\PY{c+c1}{\PYZsh{} Demostración del método de optimización}
\PY{n}{extremoInferiorIntervalo} \PY{o}{=} \PY{l+m+mf}{0.25}
\PY{n}{extremoSuperiorIntervalo} \PY{o}{=} \PY{l+m+mi}{2}
\PY{n}{minimoEnRonda} \PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{float128} \PY{o}{=} \PY{l+m+mi}{3}
\PY{k}{for} \PY{n}{rondaDeOptimizacion} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{3}\PY{p}{)}\PY{p}{:}
    \PY{n}{anchoIntervalo} \PY{o}{=} \PY{n}{extremoSuperiorIntervalo} \PY{o}{\PYZhy{}} \PY{n}{extremoInferiorIntervalo}
    \PY{n}{tiemposTomados} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{n}{valores} \PY{o}{=} \PY{n}{puntos\PYZus{}chebyscheff}\PY{p}{(}\PY{l+m+mi}{5}\PY{p}{,}\PY{n}{extremoInferiorIntervalo}\PY{p}{,}\PY{n}{extremoSuperiorIntervalo}\PY{p}{)}
    \PY{n}{f\PYZus{}prueba} \PY{o}{=} \PY{k}{lambda} \PY{n}{x}\PY{p}{:} \PY{o}{\PYZhy{}}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{/}\PY{p}{(}\PY{l+m+mi}{1}\PY{o}{+}\PY{p}{(}\PY{n}{np}\PY{o}{.}\PY{n}{tan}\PY{p}{(}\PY{n}{x}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}\PY{o}{*}\PY{o}{*}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{)}
    \PY{k}{for} \PY{n}{valor} \PY{o+ow}{in} \PY{n}{valores}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} redefinición de la función de actualización con el nuevo valor}

        \PY{c+c1}{\PYZsh{} Generación de variable con el tiempo de referencia para la nueva solución.}
        \PY{c+c1}{\PYZsh{} Solución con el nuevo valor.}

        \PY{c+c1}{\PYZsh{} Toma del tiempo de la solución.}
        \PY{n}{tiemposTomados}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{f\PYZus{}prueba}\PY{p}{(}\PY{n}{valor}\PY{p}{)}\PY{p}{)}
        \PY{k}{pass}

    \PY{c+c1}{\PYZsh{} Interpolación polinomica con los tiempos de la nueva solución.}
    
    \PY{n}{interpolacion} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{polynomial}\PY{o}{.}\PY{n}{Polynomial}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{valores}\PY{p}{,}\PY{n}{tiemposTomados}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{valores}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
    
    \PY{n}{x\PYZus{}grafica} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{extremoInferiorIntervalo}\PY{p}{,}\PY{n}{extremoSuperiorIntervalo}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{valores}\PY{p}{,}\PY{n}{tiemposTomados}\PY{p}{,}\PY{n}{linewidth} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{marker} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{*}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Puntos de prueba de la función}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{ms} \PY{o}{=} \PY{l+m+mi}{15}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x\PYZus{}grafica}\PY{p}{,}\PY{n}{f\PYZus{}prueba}\PY{p}{(}\PY{n}{x\PYZus{}grafica}\PY{p}{)}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{función original}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{linestyle} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZhy{}\PYZhy{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{linewidth}\PY{o}{=}\PY{l+m+mi}{3}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x\PYZus{}grafica}\PY{p}{,}\PY{n}{interpolacion}\PY{p}{(}\PY{n}{x\PYZus{}grafica}\PY{p}{)}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Interpolación de la función}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{n}{extremoInferiorIntervalo}\PY{p}{,}\PY{n}{extremoSuperiorIntervalo}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} minimo en ronda es el punto en el cual se encontro el minimo DENTRO del polinomio.}
    \PY{n}{minimoEnRonda} \PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{float128} \PY{o}{=} \PY{n}{x\PYZus{}grafica}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmin}\PY{p}{(}\PY{n}{interpolacion}\PY{p}{(}\PY{n}{x\PYZus{}grafica}\PY{p}{)}\PY{p}{)}\PY{p}{]}
    \PY{c+c1}{\PYZsh{} Reducción del intervalo de busqueda a un 30\PYZpc{} para la siguiente ronda de optimización.}
    \PY{n}{extremoInferiorIntervalo} \PY{o}{=} \PY{n}{minimoEnRonda} \PY{o}{\PYZhy{}} \PY{l+m+mf}{0.3}\PY{o}{/}\PY{l+m+mi}{2} \PY{o}{*} \PY{n}{anchoIntervalo}
    \PY{n}{extremoSuperiorIntervalo} \PY{o}{=} \PY{n}{minimoEnRonda} \PY{o}{+} \PY{l+m+mf}{0.3}\PY{o}{/}\PY{l+m+mi}{2} \PY{o}{*} \PY{n}{anchoIntervalo}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{minimoEnRonda}\PY{p}{,}\PY{n}{interpolacion}\PY{p}{(}\PY{n}{minimoEnRonda}\PY{p}{)}\PY{p}{,}\PY{n}{marker} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{ms} \PY{o}{=} \PY{l+m+mi}{25}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Minimo encontrado}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{linewidth} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Minimo Encontrado}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{minimoEnRonda}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{extremos}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{p}{[}\PY{n}{extremoInferiorIntervalo}\PY{p}{,}\PY{n}{extremoSuperiorIntervalo}\PY{p}{]}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{cantidad de puntos de prueba}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{valores}\PY{p}{)}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{grado del polinomio}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{interpolacion}\PY{o}{.}\PY{n}{degree}\PY{p}{(}\PY{p}{)}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Minimo Encontrado : }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{minimoEnRonda}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Minimo Teorico    : }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+m+mf}{0.7853981}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Error Porcentual  : }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{p}{(}\PY{n}{minimoEnRonda}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.7853981}\PY{p}{)}\PY{o}{/}\PY{l+m+mf}{0.7853981}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}

\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Minimo Encontrado : }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{minimoEnRonda}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Minimo Teorico    : }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{l+m+mf}{0.7853981}\PY{p}{)}
\PY{n+nb}{print}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Error Porcentual  : }\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{p}{(}\PY{n}{minimoEnRonda}\PY{o}{\PYZhy{}}\PY{l+m+mf}{0.7853981}\PY{p}{)}\PY{o}{/}\PY{l+m+mf}{0.7853981}\PY{o}{*}\PY{l+m+mi}{100}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
Minimo Encontrado 0.5714285714285714
extremos [0.3089285714285714, 0.8339285714285714]
cantidad de puntos de prueba 5
grado del polinomio 4
Minimo Encontrado :  0.5714285714285714
Minimo Teorico    :  0.7853981
Error Porcentual  :  -27.24344871364326
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Final_files/Final_23_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Minimo Encontrado 0.7803571428571427
extremos [0.7016071428571428, 0.8591071428571427]
cantidad de puntos de prueba 5
grado del polinomio 4
Minimo Encontrado :  0.7803571428571427
Minimo Teorico    :  0.7853981
Error Porcentual  :  -0.6418346495690833
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Final_files/Final_23_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Minimo Encontrado 0.7851785714285713
extremos [0.7615535714285713, 0.8088035714285713]
cantidad de puntos de prueba 5
grado del polinomio 4
Minimo Encontrado :  0.7851785714285713
Minimo Teorico    :  0.7853981
Error Porcentual  :  -0.027951248090453303
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Final_files/Final_23_5.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
Minimo Encontrado :  0.7851785714285713
Minimo Teorico    :  0.7853981
Error Porcentual  :  -0.027951248090453303
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{13}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{os}\PY{o}{.}\PY{n}{chdir}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{/home/externo/Documents/nico/efficientPINN/pythontesis}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{k}{if} \PY{n}{estaResolviendo}\PY{p}{:}
    \PY{n}{os}\PY{o}{.}\PY{n}{chdir}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{v6}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{filename} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RESULTADOS/parametros/parametrosEncontrados}\PY{l+s+s2}{\PYZdq{}}
\PY{k}{else}\PY{p}{:}
    \PY{n}{os}\PY{o}{.}\PY{n}{chdir}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{basura}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{filename} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RESULTADOS/parametros/parametrosEncontrados}\PY{l+s+s2}{\PYZdq{}}

\PY{n+nb}{print}\PY{p}{(}\PY{n}{os}\PY{o}{.}\PY{n}{getcwd}\PY{p}{(}\PY{p}{)}\PY{p}{)}
\PY{k}{try}\PY{p}{:}
    \PY{n}{archivo} \PY{o}{=} \PY{n+nb}{open}\PY{p}{(}\PY{n}{filename}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{xb}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{k}{except} \PY{n+ne}{FileExistsError}\PY{p}{:}
    \PY{n}{archivo} \PY{o}{=} \PY{n+nb}{open}\PY{p}{(}\PY{n}{filename}\PY{p}{,}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{wb}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
/home/externo/Documents/nico/efficientPINN/pythontesis/v6
    \end{Verbatim}

    \hypertarget{implementaciuxf3n-con-el-parametro-c1-y-c2}{%
\subsubsection{Implementación con el parametro C1 y
C2}\label{implementaciuxf3n-con-el-parametro-c1-y-c2}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{36}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{statSolucion}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,}\PY{n}{variable}\PY{p}{,}\PY{n}{learningRate}\PY{p}{,}\PY{n}{registro\PYZus{}perdida}\PY{p}{,}\PY{n}{registro\PYZus{}promedio}\PY{p}{,}\PY{n}{tiempo}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{variable} \PY{o}{=} \PY{n}{variable}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{learningRate} \PY{o}{=} \PY{n}{learningRate}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{registro\PYZus{}perdida} \PY{o}{=} \PY{n}{registro\PYZus{}perdida}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{registro\PYZus{}promedio} \PY{o}{=} \PY{n}{registro\PYZus{}promedio}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{tiempo} \PY{o}{=} \PY{n}{tiempo}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{epocasPorSample} \PY{o}{=} \PY{l+m+mi}{20} 

\PY{k}{def} \PY{n+nf}{descompresor\PYZus{}de\PYZus{}dominio} \PY{p}{(}\PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{p}{,}\PY{n}{max\PYZus{}x}\PY{p}{,}\PY{n}{max\PYZus{}y}\PY{p}{)}\PY{p}{:}
    \PY{n}{x} \PY{o}{=} \PY{n}{x}\PY{o}{*}\PY{p}{(}\PY{n}{max\PYZus{}x}\PY{p}{)}
    \PY{n}{x} \PY{o}{=} \PY{l+m+mi}{10}\PY{o}{*}\PY{o}{*}\PY{n}{x}
    \PY{k}{return} \PY{n}{x}\PY{p}{,}\PY{n}{y}\PY{o}{*}\PY{n}{max\PYZus{}y}
 

\PY{k}{def} \PY{n+nf}{compresor\PYZus{}de\PYZus{}dominio}\PY{p}{(}\PY{n}{x\PYZus{}exp}\PY{p}{,}\PY{n}{y\PYZus{}exp}\PY{p}{)}\PY{p}{:}
    \PY{n}{max\PYZus{}y\PYZus{}exp} \PY{o}{=} \PY{n+nb}{max}\PY{p}{(}\PY{n}{y\PYZus{}exp}\PY{p}{)}
    \PY{n}{x\PYZus{}exp} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{log10}\PY{p}{(}\PY{n}{x\PYZus{}exp}\PY{p}{)}
    \PY{n}{x\PYZus{}exp} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{x\PYZus{}exp}\PY{p}{)}
    \PY{n}{max\PYZus{}x\PYZus{}exp} \PY{o}{=} \PY{n+nb}{max}\PY{p}{(}\PY{n}{x\PYZus{}exp}\PY{p}{)}
    \PY{n}{x\PYZus{}exp} \PY{o}{=} \PY{n}{x\PYZus{}exp}\PY{o}{/}\PY{n}{max\PYZus{}x\PYZus{}exp}
    \PY{n}{y\PYZus{}exp} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{array}\PY{p}{(}\PY{n}{y\PYZus{}exp}\PY{p}{)}\PY{o}{/}\PY{n+nb}{max}\PY{p}{(}\PY{n}{y\PYZus{}exp}\PY{p}{)}
    \PY{k}{return} \PY{n}{x\PYZus{}exp}\PY{p}{,}\PY{n}{y\PYZus{}exp}\PY{p}{,}\PY{n}{max\PYZus{}x\PYZus{}exp}\PY{p}{,}\PY{n}{max\PYZus{}y\PYZus{}exp}

\PY{c+c1}{\PYZsh{} La función solucionar funciona para hacer el código de abajo más legible}
\PY{c+c1}{\PYZsh{} Esta función solo soluciona el problema enunciado en el principio.}
\PY{k}{def} \PY{n+nf}{solucionar}\PY{p}{(}\PY{n}{serializar} \PY{o}{=} \PY{k+kc}{False}\PY{p}{,}\PY{n}{variable} \PY{p}{:}\PY{n+nb}{str} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{archivo} \PY{o}{=} \PY{n}{archivo}\PY{p}{,}\PY{n}{ronda} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{n}{learningRate} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}3}\PY{p}{)}\PY{p}{:}
    \PY{c+c1}{\PYZsh{} El modulo serializar no esta listo y necesitara un archivo y un cerrador del archivo}
    \PY{n}{ronda} \PY{o}{=} \PY{n+nb}{str}\PY{p}{(}\PY{n}{ronda}\PY{p}{)}
    \PY{k}{if} \PY{n}{serializar} \PY{o}{==} \PY{k+kc}{True} \PY{o+ow}{and} \PY{n}{archivo}\PY{o}{.}\PY{n}{closed}\PY{p}{:} 
        \PY{k}{raise} \PY{n+ne}{Exception}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{El archivo en la serialización esta vacio}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{optimizer} \PY{o}{=} \PY{n}{torch}\PY{o}{.}\PY{n}{optim}\PY{o}{.}\PY{n}{Adam}\PY{p}{(}\PY{n}{redDinamica}\PY{o}{.}\PY{n}{parameters}\PY{p}{(}\PY{p}{)}\PY{p}{,} \PY{n}{lr}\PY{o}{=}\PY{n}{learningRate}\PY{p}{)}
    \PY{k}{if} \PY{n}{serializar}\PY{p}{:}
       \PY{n}{registro\PYZus{}perdida}\PY{o}{=}\PY{p}{[}\PY{p}{]}
       \PY{n}{registro\PYZus{}promedio}\PY{o}{=}\PY{p}{[}\PY{p}{]}
    \PY{n}{tiempoInicial} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
    \PY{n}{i} \PY{o}{=} \PY{l+m+mi}{0}
    \PY{n}{termino} \PY{o}{=} \PY{k+kc}{False}
    \PY{k}{while} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{n}{tiempoInicial}\PY{o}{\PYZlt{}}\PY{l+m+mi}{400} \PY{o+ow}{and} \PY{o+ow}{not} \PY{n}{termino} \PY{o+ow}{and} \PY{n}{estaResolviendo}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} Compute prediction and loss}
        \PY{n}{loss} \PY{o}{=} \PY{n}{perdidaConPesos}\PY{p}{(}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Backpropagation}
        \PY{n}{optimizer}\PY{o}{.}\PY{n}{zero\PYZus{}grad}\PY{p}{(}\PY{p}{)}
        \PY{n}{loss}\PY{o}{.}\PY{n}{backward}\PY{p}{(}\PY{p}{)}
        \PY{n}{optimizer}\PY{o}{.}\PY{n}{step}\PY{p}{(}\PY{p}{)}

        \PY{k}{if} \PY{n}{i} \PY{o}{\PYZpc{}} \PY{l+m+mi}{8} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
            \PY{n}{actualizarPuntosConPesos}\PY{p}{(}\PY{p}{)}
            \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+si}{\PYZob{}}\PY{n}{promAct}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{ }\PY{l+s+si}{\PYZob{}}\PY{n}{i}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

        \PY{k}{if} \PY{n}{i} \PY{o}{\PYZpc{}} \PY{l+m+mi}{20} \PY{o}{==} \PY{l+m+mi}{0}\PY{p}{:}
            \PY{k}{if} \PY{n}{serializar}\PY{p}{:}
                \PY{n}{registro\PYZus{}perdida}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{perdidaParaRevisar}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{puntos}\PY{p}{)}\PY{p}{)}
                \PY{n}{registro\PYZus{}promedio}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{promAct}\PY{p}{)}
            \PY{k}{else}\PY{p}{:}
                \PY{n}{inutil} \PY{o}{=} \PY{n}{perdidaParaRevisar}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{item}\PY{p}{(}\PY{p}{)}\PY{o}{/}\PY{n+nb}{len}\PY{p}{(}\PY{n}{puntos}\PY{p}{)}
            \PY{n}{termino} \PY{o}{=} \PY{n}{revisador}\PY{p}{(}\PY{p}{)}
        \PY{n}{i}\PY{o}{+}\PY{o}{=}\PY{l+m+mi}{1}
    \PY{k}{if} \PY{n}{termino}\PY{p}{:}
        \PY{n}{tiempo} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{n}{tiempoInicial}
    \PY{k}{else}\PY{p}{:}
        \PY{n}{tiempo} \PY{o}{=} \PY{o}{\PYZhy{}}\PY{l+m+mi}{1}

    \PY{k}{if} \PY{n}{serializar}\PY{p}{:}
        \PY{n}{nombreParaGuardarRedFinal} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RESULTADOS/parametros/redesIntermedias/senoParalelo}\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{n+nb}{str}\PY{p}{(}\PY{n}{i}\PY{p}{)}\PY{o}{+}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ epochs variable }\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{n}{variable}\PY{o}{+}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{ronda}\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{n}{ronda} \PY{o}{+}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{.tar}\PY{l+s+s2}{\PYZdq{}}
        \PY{n}{torch}\PY{o}{.}\PY{n}{save}\PY{p}{(}\PY{n}{redDinamica}\PY{o}{.}\PY{n}{state\PYZus{}dict}\PY{p}{(}\PY{p}{)}\PY{p}{,}\PY{n}{nombreParaGuardarRedFinal}\PY{p}{)}
        \PY{n}{estadisticasSolucion} \PY{o}{=} \PY{n}{statSolucion}\PY{p}{(}\PY{n}{variable} \PY{o}{=} \PY{n}{variable}\PY{p}{,}
                                            \PY{n}{learningRate} \PY{o}{=} \PY{n}{learningRate}\PY{p}{,}
                                            \PY{n}{registro\PYZus{}perdida}\PY{o}{=}\PY{n}{registro\PYZus{}perdida}\PY{p}{,}
                                            \PY{n}{registro\PYZus{}promedio}\PY{o}{=}\PY{n}{registro\PYZus{}promedio}\PY{p}{,}
                                            \PY{n}{tiempo}\PY{o}{=}\PY{n}{tiempo}\PY{p}{)}
        \PY{n}{pickle}\PY{o}{.}\PY{n}{dump}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{estadisticas de la solución}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{archivo}\PY{p}{)}
        \PY{n}{pickle}\PY{o}{.}\PY{n}{dump}\PY{p}{(}\PY{n}{estadisticasSolucion}\PY{p}{,}\PY{n}{archivo}\PY{p}{)}
        \PY{k}{return} \PY{n}{estadisticasSolucion}
    \PY{k}{else}\PY{p}{:}
        \PY{k}{return} \PY{k+kc}{None}
\PY{n}{estaResolviendo} \PY{o}{=} \PY{k+kc}{True}
\PY{n}{c1} \PY{o}{=} \PY{l+m+mf}{1e\PYZhy{}6}
\PY{n}{c2} \PY{o}{=} \PY{l+m+mf}{1.6}
\PY{n}{sigma} \PY{o}{=} \PY{l+m+mf}{0.5}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{37}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{optimizacionParametro}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,}\PY{n}{variable}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{intervalos} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{minimos} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{soluciones} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{polinomios} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{puntosPolTiempo} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{puntosPolX} \PY{o}{=} \PY{p}{[}\PY{p}{]}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{variable} \PY{o}{=} \PY{n}{variable}
    
    \PY{k}{def} \PY{n+nf}{anadir}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,}\PY{n}{intervalo}\PY{p}{,}\PY{n}{minimo}\PY{p}{,}\PY{n}{polinomio}\PY{p}{,}\PY{n}{statSol}\PY{p}{,}\PY{n}{puntoPolTiempo}\PY{p}{,}\PY{n}{puntoPolX}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{intervalos}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{intervalo}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{minimos}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{minimo}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{polinomios}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{polinomio}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{soluciones}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{statSol}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{puntosPolTiempo}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{puntoPolTiempo}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{puntosPolX}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{puntoPolX}\PY{p}{)}
        
\PY{n}{estadisticasC1} \PY{o}{=} \PY{n}{optimizacionParametro}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{C1}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{estadisticasC2} \PY{o}{=} \PY{n}{optimizacionParametro}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{C2}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\PY{n}{estadisticassigma}\PY{o}{=} \PY{n}{optimizacionParametro}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{sigma}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    C1

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{extremoInferiorIntervalo} \PY{o}{=} \PY{l+m+mf}{0.25e\PYZhy{}6}
\PY{n}{extremoSuperiorIntervalo} \PY{o}{=} \PY{l+m+mf}{5e\PYZhy{}5}
\PY{n}{minimoEnRonda} \PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{float128} \PY{o}{=} \PY{l+m+mi}{3}
\PY{k}{for} \PY{n}{rondaDeOptimizacion} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{:}
    \PY{n}{anchoIntervalo} \PY{o}{=} \PY{n}{extremoSuperiorIntervalo} \PY{o}{\PYZhy{}} \PY{n}{extremoInferiorIntervalo}
    \PY{n}{tiemposTomados} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{n}{valores\PYZus{}logaritmicos} \PY{o}{=} \PY{n}{puntos\PYZus{}chebyscheff}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{log10}\PY{p}{(}\PY{n}{extremoInferiorIntervalo}\PY{p}{)}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{log10}\PY{p}{(}\PY{n}{extremoSuperiorIntervalo}\PY{p}{)}\PY{p}{)}
    \PY{n}{valores} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{power}\PY{p}{(}\PY{l+m+mi}{10}\PY{p}{,}\PY{n}{valores\PYZus{}logaritmicos}\PY{p}{)}
    \PY{n}{estadisticasDeSolucionesEnPDC}\PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{c+c1}{\PYZsh{} está solución solo es para quemar el primer valor}
    \PY{n}{solucionar}\PY{p}{(}\PY{n}{variable} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Dummy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{serializar} \PY{o}{=} \PY{k+kc}{True}\PY{p}{)}
    \PY{k}{for} \PY{n}{valor} \PY{o+ow}{in} \PY{n}{valores}\PY{p}{:}
        \PY{c+c1}{\PYZsh{} redefinición de la función de actualización con el nuevo valor}
        \PY{n}{c1} \PY{o}{=} \PY{n}{valor}
        \PY{c+c1}{\PYZsh{} Generación de variable con el tiempo de referencia para la nueva solución.}
        \PY{n}{t0} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Reestablecimiento de los parametros de la red Neuronal}
        \PY{n}{redDinamica} \PY{o}{=} \PY{n}{NeuralNetworkPrueba}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Solución con el nuevo valor.}
        \PY{n}{estadisticasDeSolucionesEnPDC}\PY{o}{.}\PY{n}{append}\PY{p}{(} \PY{n}{solucionar}\PY{p}{(}\PY{n}{variable} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{C1}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{serializar}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Toma del tiempo de la solución.}
        \PY{n}{tiemposTomados}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{n}{t0}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{valores : }\PY{l+s+si}{\PYZob{}}\PY{n}{valores}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n+nb}{print}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tiempos tomados : }\PY{l+s+si}{\PYZob{}}\PY{n}{tiemposTomados}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
    \PY{n}{x\PYZus{}exp}\PY{p}{,}\PY{n}{y\PYZus{}exp}\PY{p}{,}\PY{n}{max\PYZus{}x\PYZus{}exp}\PY{p}{,}\PY{n}{max\PYZus{}y\PYZus{}exp} \PY{o}{=} \PY{n}{compresor\PYZus{}de\PYZus{}dominio}\PY{p}{(}\PY{n}{valores}\PY{p}{,}\PY{n}{tiemposTomados}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} Interpolación polinomica con los tiempos de la nueva solución.}
    \PY{n}{interpolacion} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{polynomial}\PY{o}{.}\PY{n}{Polynomial}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{x\PYZus{}exp}\PY{p}{,}\PY{n}{y\PYZus{}exp}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{valores}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
    \PY{n}{x\PYZus{}interpolacion\PYZus{}comprimida} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n+nb}{min}\PY{p}{(}\PY{n}{x\PYZus{}exp}\PY{p}{)}\PY{p}{,}\PY{n+nb}{max}\PY{p}{(}\PY{n}{x\PYZus{}exp}\PY{p}{)}\PY{p}{,}\PY{l+m+mi}{200}\PY{p}{)}
    \PY{n}{y\PYZus{}interpolacion\PYZus{}comprimida} \PY{o}{=} \PY{n}{interpolacion}\PY{p}{(}\PY{n}{x\PYZus{}interpolacion\PYZus{}comprimida}\PY{p}{)}
    \PY{n}{x\PYZus{}interpolacion}\PY{p}{,}\PY{n}{y\PYZus{}interpolacion} \PY{o}{=} \PY{n}{descompresor\PYZus{}de\PYZus{}dominio}\PY{p}{(}\PY{n}{x\PYZus{}interpolacion\PYZus{}comprimida}\PY{p}{,}\PY{n}{y\PYZus{}interpolacion\PYZus{}comprimida}\PY{p}{,}\PY{n}{max\PYZus{}x\PYZus{}exp}\PY{p}{,}\PY{n}{max\PYZus{}y\PYZus{}exp}\PY{p}{)}
    \PY{c+c1}{\PYZsh{}Generación de un eje X para las gráficas y obetner el estimado del valor minimo.}

    \PY{c+c1}{\PYZsh{} minimo en ronda es el punto en el cual se encontro el minimo DENTRO del polinomio.}
    \PY{n}{ubmin} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{argmin}\PY{p}{(}\PY{n}{y\PYZus{}interpolacion}\PY{p}{)}
    \PY{n}{minimoEnRonda} \PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{float128} \PY{o}{=} \PY{n}{x\PYZus{}interpolacion}\PY{p}{[}\PY{n}{ubmin}\PY{p}{]}
    \PY{n}{c1} \PY{o}{=} \PY{n}{minimoEnRonda}

    \PY{n}{plt}\PY{o}{.}\PY{n}{semilogx}\PY{p}{(}\PY{n}{valores}\PY{p}{,}\PY{n}{tiemposTomados}\PY{p}{,}\PY{n}{linewidth} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{marker} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{*}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Puntos de prueba de la función}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{semilogx}\PY{p}{(}\PY{n}{x\PYZus{}interpolacion}\PY{p}{,}\PY{n}{y\PYZus{}interpolacion}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Interpolación de la función}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{n}{extremoInferiorIntervalo}\PY{p}{,}\PY{n}{extremoSuperiorIntervalo}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{semilogx}\PY{p}{(}\PY{n}{x\PYZus{}interpolacion}\PY{p}{[}\PY{n}{ubmin}\PY{p}{]}\PY{p}{,}\PY{n}{y\PYZus{}interpolacion}\PY{p}{[}\PY{n}{ubmin}\PY{p}{]}\PY{p}{,}\PY{n}{marker} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{+}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{ms} \PY{o}{=} \PY{l+m+mi}{5}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Minimo encontrado}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Optimizacion de C1, Ronda de optimización }\PY{l+s+si}{\PYZob{}}\PY{n}{rondaDeOptimizacion}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{nombreGuardar} \PY{o}{=} \PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{c1, int }\PY{l+s+si}{\PYZob{}}\PY{n}{rondaDeOptimizacion}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}
    \PY{c+c1}{\PYZsh{}plt.savefig(\PYZdq{}RESULTADOS/parametros/graficas/\PYZdq{}+nombreGuardar,dpi = 300)}

    \PY{c+c1}{\PYZsh{} Serialización de la solución}
    \PY{n}{estadisticasC1}\PY{o}{.}\PY{n}{anadir}\PY{p}{(}\PY{p}{[}\PY{n}{extremoInferiorIntervalo}\PY{p}{,}\PY{n}{extremoSuperiorIntervalo}\PY{p}{]}\PY{p}{,}
                            \PY{n}{minimoEnRonda}\PY{p}{,}
                            \PY{n}{interpolacion}\PY{p}{,}
                            \PY{n}{estadisticasDeSolucionesEnPDC}\PY{p}{,}
                            \PY{n}{tiemposTomados}\PY{p}{,}
                            \PY{n}{valores}\PY{p}{)}



    \PY{c+c1}{\PYZsh{} Reducción del intervalo de busqueda a un 20\PYZpc{} para la siguiente ronda de optimización.}
    \PY{n}{extremoInferiorIntervalo} \PY{o}{=} \PY{n}{minimoEnRonda} \PY{o}{\PYZhy{}} \PY{l+m+mf}{0.2}\PY{o}{/}\PY{l+m+mi}{2} \PY{o}{*} \PY{n}{anchoIntervalo}
    \PY{n}{extremoSuperiorIntervalo} \PY{o}{=} \PY{n}{minimoEnRonda} \PY{o}{+} \PY{l+m+mf}{0.2}\PY{o}{/}\PY{l+m+mi}{2} \PY{o}{*} \PY{n}{anchoIntervalo}
    
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{c2}{%
\subsection{C2}\label{c2}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{17}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Optimización para el parametro C2}
\PY{n}{c1} \PY{o}{=} \PY{l+m+mf}{1.1e\PYZhy{}6}
\PY{n}{extremoInferiorIntervalo} \PY{o}{=} \PY{l+m+mf}{0.8}
\PY{n}{extremoSuperiorIntervalo} \PY{o}{=} \PY{l+m+mi}{2}
\PY{n}{minimoEnRonda} \PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{float128} \PY{o}{=} \PY{l+m+mi}{3}
\PY{n}{solucionar}\PY{p}{(}\PY{n}{variable} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Dummy}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{serializar} \PY{o}{=} \PY{k+kc}{True}\PY{p}{)}
\PY{k}{for} \PY{n}{rondaDeOptimizacion} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{:}
    \PY{n}{anchoIntervalo} \PY{o}{=} \PY{n}{extremoSuperiorIntervalo} \PY{o}{\PYZhy{}} \PY{n}{extremoInferiorIntervalo}
    \PY{n}{tiemposTomados} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{n}{valores} \PY{o}{=} \PY{n}{puntos\PYZus{}chebyscheff}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{n}{extremoInferiorIntervalo}\PY{p}{,}\PY{n}{extremoSuperiorIntervalo}\PY{p}{)}
    \PY{n}{estadisticasDeSolucionesEnPDC}\PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{k}{for} \PY{n}{valor} \PY{o+ow}{in} \PY{n}{valores}\PY{p}{:}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{valor}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} redefinición de la función de actualización con el nuevo valor}
        \PY{n}{c2} \PY{o}{=} \PY{n}{valor}
        \PY{c+c1}{\PYZsh{} Generación de variable con el tiempo de referencia para la nueva solución.}
        \PY{n}{t0} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Reestablecimiento de los parametros de la red Neuronal}
        \PY{n}{redDinamica} \PY{o}{=} \PY{n}{NeuralNetworkPrueba}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Reestablecimientos de los puntos de prueba}
        \PY{n}{inicializarPuntos}\PY{p}{(}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Solución con el nuevo valor.}
        \PY{n}{estadisticasDeSolucionesEnPDC}\PY{o}{.}\PY{n}{append}\PY{p}{(} \PY{n}{solucionar}\PY{p}{(}\PY{n}{serializar}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Toma del tiempo de la solución.}
        \PY{n}{tiemposTomados}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{n}{t0}\PY{p}{)}

        \PY{n+nb}{print}\PY{p}{(}\PY{n}{promAct}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{n}{t0}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Interpolación polinomica con los tiempos de la nueva solución.}
    \PY{n}{interpolacion} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{polynomial}\PY{o}{.}\PY{n}{Polynomial}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{valores}\PY{p}{,}\PY{n}{tiemposTomados}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{valores}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{}Generación de un eje X para las gráficas y obetner el estimado del valor minimo.}
    \PY{n}{x\PYZus{}grafica} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{extremoInferiorIntervalo}\PY{p}{,}\PY{n}{extremoSuperiorIntervalo}\PY{p}{,}\PY{l+m+mi}{200}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} minimo en ronda es el punto en el cual se encontro el minimo DENTRO del polinomio.}
    \PY{n}{minimoEnRonda} \PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{float128} \PY{o}{=} \PY{n}{x\PYZus{}grafica}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmin}\PY{p}{(}\PY{n}{interpolacion}\PY{p}{(}\PY{n}{x\PYZus{}grafica}\PY{p}{)}\PY{p}{)}\PY{p}{]}
    \PY{n}{c2} \PY{o}{=} \PY{n}{minimoEnRonda}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{valores}\PY{p}{,}\PY{n}{tiemposTomados}\PY{p}{,}\PY{n}{linewidth} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{marker} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{*}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Puntos de prueba de la función}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x\PYZus{}grafica}\PY{p}{,}\PY{n}{interpolacion}\PY{p}{(}\PY{n}{x\PYZus{}grafica}\PY{p}{)}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Interpolación de la función}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{n}{extremoInferiorIntervalo}\PY{p}{,}\PY{n}{extremoSuperiorIntervalo}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{minimoEnRonda}\PY{p}{,}\PY{n}{interpolacion}\PY{p}{(}\PY{n}{minimoEnRonda}\PY{p}{)}\PY{p}{,}\PY{n}{marker} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{+}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{ms} \PY{o}{=} \PY{l+m+mi}{10}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Minimo encontrado}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{c2, Ronda de optimización interna}\PY{l+s+si}{\PYZob{}}\PY{n}{rondaDeOptimizacion}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{nombreGuardar} \PY{o}{=} \PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{c2, int }\PY{l+s+si}{\PYZob{}}\PY{n}{rondaDeOptimizacion}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}
    \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{savefig}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RESULTADOS/parametros/graficas/}\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{n}{nombreGuardar}\PY{p}{,}\PY{n}{dpi} \PY{o}{=} \PY{l+m+mi}{300}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Serialización de la solución}
    \PY{n}{estadisticasC2}\PY{o}{.}\PY{n}{anadir}\PY{p}{(}\PY{p}{[}\PY{n}{extremoInferiorIntervalo}\PY{p}{,}\PY{n}{extremoSuperiorIntervalo}\PY{p}{]}\PY{p}{,}
                            \PY{n}{minimoEnRonda}\PY{p}{,}
                            \PY{n}{interpolacion}\PY{p}{,}
                            \PY{n}{estadisticasDeSolucionesEnPDC}\PY{p}{,}
                            \PY{n}{tiemposTomados}\PY{p}{,}
                            \PY{n}{valores}\PY{p}{)}



    \PY{c+c1}{\PYZsh{} Reducción del intervalo de busqueda a un 20\PYZpc{} para la siguiente ronda de optimización.}
    \PY{n}{extremoInferiorIntervalo} \PY{o}{=} \PY{n}{minimoEnRonda} \PY{o}{*} \PY{l+m+mf}{1.1}
    \PY{n}{extremoSuperiorIntervalo} \PY{o}{=} \PY{n}{minimoEnRonda} \PY{o}{*} \PY{l+m+mf}{0.9}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
tensor([0.1218], device='cuda:0', grad\_fn=<AddBackward0>)
400.0909368991852
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Final_files/Final_31_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \begin{Verbatim}[commandchars=\\\{\}]
    \end{Verbatim}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{14}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{} Optimización para el parametro C2}
\PY{n}{c1} \PY{o}{=} \PY{l+m+mf}{1.1e\PYZhy{}6}
\PY{n}{extremoInferiorIntervalo} \PY{o}{=} \PY{l+m+mf}{1.1}
\PY{n}{extremoSuperiorIntervalo} \PY{o}{=} \PY{l+m+mf}{1.9}
\PY{n}{minimoEnRonda} \PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{float128} \PY{o}{=} \PY{l+m+mi}{3}\PY{c+c1}{\PYZsh{} Optimización para el parametro C2}
\PY{n}{c1} \PY{o}{=} \PY{l+m+mf}{1.1e\PYZhy{}6}
\PY{n}{extremoInferiorIntervalo} \PY{o}{=} \PY{l+m+mf}{1.1}
\PY{n}{extremoSuperiorIntervalo} \PY{o}{=} \PY{l+m+mf}{1.9}
\PY{n}{minimoEnRonda} \PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{float128} \PY{o}{=} \PY{l+m+mi}{3}
\PY{c+c1}{\PYZsh{}solucionar(variable = \PYZsq{}Dummy\PYZsq{}, serializar = True)}
\PY{k}{for} \PY{n}{rondaDeOptimizacion} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
    \PY{n}{anchoIntervalo} \PY{o}{=} \PY{n}{extremoSuperiorIntervalo} \PY{o}{\PYZhy{}} \PY{n}{extremoInferiorIntervalo}
    \PY{n}{tiemposTomados} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{n}{valores} \PY{o}{=} \PY{n}{puntos\PYZus{}chebyscheff}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{n}{extremoInferiorIntervalo}\PY{p}{,}\PY{n}{extremoSuperiorIntervalo}\PY{p}{)}
    \PY{n}{estadisticasDeSolucionesEnPDC}\PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{k}{for} \PY{n}{valor} \PY{o+ow}{in} \PY{n}{valores}\PY{p}{:}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{valor}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} redefinición de la función de actualización con el nuevo valor}
        \PY{n}{c2} \PY{o}{=} \PY{n}{valor}
        \PY{c+c1}{\PYZsh{} Generación de variable con el tiempo de referencia para la nueva solución.}
        \PY{n}{t0} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Reestablecimiento de los parametros de la red Neuronal}
        \PY{n}{redDinamica} \PY{o}{=} \PY{n}{NeuralNetworkPrueba}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Reestablecimientos de los puntos de prueba}
        \PY{n}{inicializarPuntos}\PY{p}{(}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Solución con el nuevo valor.}
        \PY{n}{estadisticasDeSolucionesEnPDC}\PY{o}{.}\PY{n}{append}\PY{p}{(} \PY{n}{solucionar}\PY{p}{(}\PY{n}{serializar}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Toma del tiempo de la solución.}
        \PY{n}{tiemposTomados}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{n}{t0}\PY{p}{)}

        \PY{n+nb}{print}\PY{p}{(}\PY{n}{promAct}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{n}{t0}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Interpolación polinomica con los tiempos de la nueva solución.}
    \PY{n}{interpolacion} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{polynomial}\PY{o}{.}\PY{n}{Polynomial}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{valores}\PY{p}{,}\PY{n}{tiemposTomados}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{valores}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{}Generación de un eje X para las gráficas y obetner el estimado del valor minimo.}
    \PY{n}{x\PYZus{}grafica} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{extremoInferiorIntervalo}\PY{p}{,}\PY{n}{extremoSuperiorIntervalo}\PY{p}{,}\PY{l+m+mi}{200}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} minimo en ronda es el punto en el cual se encontro el minimo DENTRO del polinomio.}
    \PY{n}{minimoEnRonda} \PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{float128} \PY{o}{=} \PY{n}{x\PYZus{}grafica}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmin}\PY{p}{(}\PY{n}{interpolacion}\PY{p}{(}\PY{n}{x\PYZus{}grafica}\PY{p}{)}\PY{p}{)}\PY{p}{]}
    \PY{n}{c2} \PY{o}{=} \PY{n}{minimoEnRonda}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{valores}\PY{p}{,}\PY{n}{tiemposTomados}\PY{p}{,}\PY{n}{linewidth} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{marker} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{*}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Puntos de prueba de la función}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x\PYZus{}grafica}\PY{p}{,}\PY{n}{interpolacion}\PY{p}{(}\PY{n}{x\PYZus{}grafica}\PY{p}{)}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Interpolación de la función}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{n}{extremoInferiorIntervalo}\PY{p}{,}\PY{n}{extremoSuperiorIntervalo}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{minimoEnRonda}\PY{p}{,}\PY{n}{interpolacion}\PY{p}{(}\PY{n}{minimoEnRonda}\PY{p}{)}\PY{p}{,}\PY{n}{marker} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{+}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{ms} \PY{o}{=} \PY{l+m+mi}{10}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Minimo encontrado}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{c2, Ronda de optimización interna}\PY{l+s+si}{\PYZob{}}\PY{n}{rondaDeOptimizacion}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{nombreGuardar} \PY{o}{=} \PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{c2, int }\PY{l+s+si}{\PYZob{}}\PY{n}{rondaDeOptimizacion}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}
    \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{savefig}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RESULTADOS/parametros/graficas/}\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{n}{nombreGuardar}\PY{p}{,}\PY{n}{dpi} \PY{o}{=} \PY{l+m+mi}{300}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Serialización de la solución}
    \PY{n}{estadisticasC2}\PY{o}{.}\PY{n}{anadir}\PY{p}{(}\PY{p}{[}\PY{n}{extremoInferiorIntervalo}\PY{p}{,}\PY{n}{extremoSuperiorIntervalo}\PY{p}{]}\PY{p}{,}
                            \PY{n}{minimoEnRonda}\PY{p}{,}
                            \PY{n}{interpolacion}\PY{p}{,}
                            \PY{n}{estadisticasDeSolucionesEnPDC}\PY{p}{,}
                            \PY{n}{tiemposTomados}\PY{p}{,}
                            \PY{n}{valores}\PY{p}{)}



    \PY{c+c1}{\PYZsh{} Reducción del intervalo de busqueda a un 20\PYZpc{} para la siguiente ronda de optimización.}
    \PY{n}{extremoInferiorIntervalo} \PY{o}{=} \PY{n}{minimoEnRonda} \PY{o}{*} \PY{l+m+mf}{1.1}
    \PY{n}{extremoSuperiorIntervalo} \PY{o}{=} \PY{n}{minimoEnRonda} \PY{o}{*} \PY{l+m+mf}{0.9}
\PY{c+c1}{\PYZsh{}solucionar(variable = \PYZsq{}Dummy\PYZsq{}, serializar = True)}
\PY{k}{for} \PY{n}{rondaDeOptimizacion} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{1}\PY{p}{)}\PY{p}{:}
    \PY{n}{anchoIntervalo} \PY{o}{=} \PY{n}{extremoSuperiorIntervalo} \PY{o}{\PYZhy{}} \PY{n}{extremoInferiorIntervalo}
    \PY{n}{tiemposTomados} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{n}{valores} \PY{o}{=} \PY{n}{puntos\PYZus{}chebyscheff}\PY{p}{(}\PY{l+m+mi}{6}\PY{p}{,}\PY{n}{extremoInferiorIntervalo}\PY{p}{,}\PY{n}{extremoSuperiorIntervalo}\PY{p}{)}
    \PY{n}{estadisticasDeSolucionesEnPDC}\PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{k}{for} \PY{n}{valor} \PY{o+ow}{in} \PY{n}{valores}\PY{p}{:}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{valor}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} redefinición de la función de actualización con el nuevo valor}
        \PY{n}{c2} \PY{o}{=} \PY{n}{valor}
        \PY{c+c1}{\PYZsh{} Generación de variable con el tiempo de referencia para la nueva solución.}
        \PY{n}{t0} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Reestablecimiento de los parametros de la red Neuronal}
        \PY{n}{redDinamica} \PY{o}{=} \PY{n}{NeuralNetworkPrueba}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Reestablecimientos de los puntos de prueba}
        \PY{n}{inicializarPuntos}\PY{p}{(}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Solución con el nuevo valor.}
        \PY{n}{estadisticasDeSolucionesEnPDC}\PY{o}{.}\PY{n}{append}\PY{p}{(} \PY{n}{solucionar}\PY{p}{(}\PY{n}{serializar}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Toma del tiempo de la solución.}
        \PY{n}{tiemposTomados}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{n}{t0}\PY{p}{)}

        \PY{n+nb}{print}\PY{p}{(}\PY{n}{promAct}\PY{p}{)}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{n}{t0}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Interpolación polinomica con los tiempos de la nueva solución.}
    \PY{n}{interpolacion} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{polynomial}\PY{o}{.}\PY{n}{Polynomial}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{valores}\PY{p}{,}\PY{n}{tiemposTomados}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{valores}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{}Generación de un eje X para las gráficas y obetner el estimado del valor minimo.}
    \PY{n}{x\PYZus{}grafica} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{extremoInferiorIntervalo}\PY{p}{,}\PY{n}{extremoSuperiorIntervalo}\PY{p}{,}\PY{l+m+mi}{200}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} minimo en ronda es el punto en el cual se encontro el minimo DENTRO del polinomio.}
    \PY{n}{minimoEnRonda} \PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{float128} \PY{o}{=} \PY{n}{x\PYZus{}grafica}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmin}\PY{p}{(}\PY{n}{interpolacion}\PY{p}{(}\PY{n}{x\PYZus{}grafica}\PY{p}{)}\PY{p}{)}\PY{p}{]}
    \PY{n}{c2} \PY{o}{=} \PY{n}{minimoEnRonda}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{valores}\PY{p}{,}\PY{n}{tiemposTomados}\PY{p}{,}\PY{n}{linewidth} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{marker} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{*}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Puntos de prueba de la función}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x\PYZus{}grafica}\PY{p}{,}\PY{n}{interpolacion}\PY{p}{(}\PY{n}{x\PYZus{}grafica}\PY{p}{)}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Interpolación de la función}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{n}{extremoInferiorIntervalo}\PY{p}{,}\PY{n}{extremoSuperiorIntervalo}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{minimoEnRonda}\PY{p}{,}\PY{n}{interpolacion}\PY{p}{(}\PY{n}{minimoEnRonda}\PY{p}{)}\PY{p}{,}\PY{n}{marker} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{+}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{ms} \PY{o}{=} \PY{l+m+mi}{10}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Minimo encontrado}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{c2, Ronda de optimización interna}\PY{l+s+si}{\PYZob{}}\PY{n}{rondaDeOptimizacion}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{nombreGuardar} \PY{o}{=} \PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{c2, int }\PY{l+s+si}{\PYZob{}}\PY{n}{rondaDeOptimizacion}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}
    \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{savefig}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RESULTADOS/parametros/graficas/}\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{n}{nombreGuardar}\PY{p}{,}\PY{n}{dpi} \PY{o}{=} \PY{l+m+mi}{300}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Serialización de la solución}
    \PY{n}{estadisticasC2}\PY{o}{.}\PY{n}{anadir}\PY{p}{(}\PY{p}{[}\PY{n}{extremoInferiorIntervalo}\PY{p}{,}\PY{n}{extremoSuperiorIntervalo}\PY{p}{]}\PY{p}{,}
                            \PY{n}{minimoEnRonda}\PY{p}{,}
                            \PY{n}{interpolacion}\PY{p}{,}
                            \PY{n}{estadisticasDeSolucionesEnPDC}\PY{p}{,}
                            \PY{n}{tiemposTomados}\PY{p}{,}
                            \PY{n}{valores}\PY{p}{)}



    \PY{c+c1}{\PYZsh{} Reducción del intervalo de busqueda a un 20\PYZpc{} para la siguiente ronda de optimización.}
    \PY{n}{extremoInferiorIntervalo} \PY{o}{=} \PY{n}{minimoEnRonda} \PY{o}{*} \PY{l+m+mf}{1.1}
    \PY{n}{extremoSuperiorIntervalo} \PY{o}{=} \PY{n}{minimoEnRonda} \PY{o}{*} \PY{l+m+mf}{0.9}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
1.1136296694843728
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Final_files/Final_32_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{sigma}{%
\subsection{\texorpdfstring{\(\sigma\)}{\textbackslash sigma}}\label{sigma}}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{43}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{extremoInferiorIntervalo} \PY{o}{=} \PY{l+m+mf}{0.05}
\PY{n}{extremoSuperiorIntervalo} \PY{o}{=} \PY{l+m+mi}{2}
\PY{n}{minimoEnRonda} \PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{float128} \PY{o}{=} \PY{l+m+mi}{2}
\PY{n}{c1} \PY{o}{=} \PY{l+m+mf}{1.1e\PYZhy{}6}
\PY{n}{c2} \PY{o}{=} \PY{l+m+mf}{1.6668341}
\PY{c+c1}{\PYZsh{}solucionar(variable = \PYZsq{}Dummy\PYZsq{}, serializar = True)}
\PY{k}{for} \PY{n}{rondaDeOptimizacion} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{l+m+mi}{2}\PY{p}{)}\PY{p}{:}
    \PY{n}{anchoIntervalo} \PY{o}{=} \PY{n}{extremoSuperiorIntervalo} \PY{o}{\PYZhy{}} \PY{n}{extremoInferiorIntervalo}
    \PY{n}{tiemposTomados} \PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{n}{valores} \PY{o}{=} \PY{n}{puntos\PYZus{}chebyscheff}\PY{p}{(}\PY{l+m+mi}{8}\PY{p}{,}\PY{n}{extremoInferiorIntervalo}\PY{p}{,}\PY{n}{extremoSuperiorIntervalo}\PY{p}{)}
    \PY{n}{estadisticasDeSolucionesEnPDC}\PY{o}{=} \PY{p}{[}\PY{p}{]}
    \PY{k}{for} \PY{n}{valor} \PY{o+ow}{in} \PY{n}{valores}\PY{p}{:}
        \PY{n+nb}{print}\PY{p}{(}\PY{n}{valor}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} redefinición de la función de actualización con el nuevo valor}
        \PY{n}{sigma} \PY{o}{=} \PY{n}{valor}
        \PY{c+c1}{\PYZsh{} Generación de variable con el tiempo de referencia para la nueva solución.}
        \PY{n}{t0} \PY{o}{=} \PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Reestablecimiento de los parametros de la red Neuronal}
        \PY{n}{inicializarPuntos}\PY{p}{(}\PY{p}{)}
        \PY{n}{promAct} \PY{o}{=} \PY{l+m+mf}{0.1}
        \PY{n}{redDinamica} \PY{o}{=} \PY{n}{NeuralNetworkPrueba}\PY{p}{(}\PY{p}{)}\PY{o}{.}\PY{n}{to}\PY{p}{(}\PY{n}{device}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Solución con el nuevo valor.}
        \PY{n}{estadisticasDeSolucionesEnPDC}\PY{o}{.}\PY{n}{append}\PY{p}{(} \PY{n}{solucionar}\PY{p}{(}\PY{n}{serializar}\PY{o}{=}\PY{k+kc}{True}\PY{p}{)}\PY{p}{)}
        \PY{c+c1}{\PYZsh{} Toma del tiempo de la solución.}
        \PY{n}{tiemposTomados}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{time}\PY{o}{.}\PY{n}{time}\PY{p}{(}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{n}{t0}\PY{p}{)}
    \PY{c+c1}{\PYZsh{} Interpolación polinomica con los tiempos de la nueva solución.}
    \PY{n}{interpolacion} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{polynomial}\PY{o}{.}\PY{n}{Polynomial}\PY{o}{.}\PY{n}{fit}\PY{p}{(}\PY{n}{valores}\PY{p}{,}\PY{n}{tiemposTomados}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{valores}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{)}
    
    \PY{c+c1}{\PYZsh{}Generación de un eje X para las gráficas y obetner el estimado del valor minimo.}
    \PY{n}{x\PYZus{}grafica} \PY{o}{=} \PY{n}{np}\PY{o}{.}\PY{n}{linspace}\PY{p}{(}\PY{n}{extremoInferiorIntervalo}\PY{p}{,}\PY{n}{extremoSuperiorIntervalo}\PY{p}{,}\PY{l+m+mi}{200}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} minimo en ronda es el punto en el cual se encontro el minimo DENTRO del polinomio.}
    \PY{n}{minimoEnRonda} \PY{p}{:} \PY{n}{np}\PY{o}{.}\PY{n}{float128} \PY{o}{=} \PY{n}{x\PYZus{}grafica}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{argmin}\PY{p}{(}\PY{n}{interpolacion}\PY{p}{(}\PY{n}{x\PYZus{}grafica}\PY{p}{)}\PY{p}{)}\PY{p}{]}
    \PY{n}{sigma} \PY{o}{=} \PY{l+m+mi}{10}\PY{o}{*}\PY{o}{*}\PY{n}{minimoEnRonda}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{valores}\PY{p}{,}\PY{n}{tiemposTomados}\PY{p}{,}\PY{n}{linewidth} \PY{o}{=} \PY{l+m+mi}{0}\PY{p}{,} \PY{n}{marker} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{*}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Puntos de prueba de la función}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{x\PYZus{}grafica}\PY{p}{,}\PY{n}{interpolacion}\PY{p}{(}\PY{n}{x\PYZus{}grafica}\PY{p}{)}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Interpolación de la función}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{xlim}\PY{p}{(}\PY{n}{extremoInferiorIntervalo}\PY{p}{,}\PY{n}{extremoSuperiorIntervalo}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{n}{minimoEnRonda}\PY{p}{,}\PY{n}{interpolacion}\PY{p}{(}\PY{n}{minimoEnRonda}\PY{p}{)}\PY{p}{,}\PY{n}{marker} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{+}\PY{l+s+s2}{\PYZdq{}}\PY{p}{,} \PY{n}{ms} \PY{o}{=} \PY{l+m+mi}{5}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{Minimo encontrado}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{sigma, Ronda de optimización interna}\PY{l+s+si}{\PYZob{}}\PY{n}{rondaDeOptimizacion}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{ , Ronda de optimización externa}\PY{l+s+s2}{\PYZdq{}}\PY{p}{)}
    \PY{n}{nombreGuardar} \PY{o}{=} \PY{l+s+sa}{f}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{sigma, int }\PY{l+s+si}{\PYZob{}}\PY{n}{rondaDeOptimizacion}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s2}{\PYZdq{}}
    \PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{savefig}\PY{p}{(}\PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{RESULTADOS/parametros/graficas/}\PY{l+s+s2}{\PYZdq{}}\PY{o}{+}\PY{n}{nombreGuardar}\PY{p}{,}\PY{n}{dpi} \PY{o}{=} \PY{l+m+mi}{300}\PY{p}{)}
    \PY{n}{plt}\PY{o}{.}\PY{n}{show}\PY{p}{(}\PY{p}{)}

    \PY{c+c1}{\PYZsh{} Serialización de la solución}
    \PY{n}{estadisticassigma}\PY{o}{.}\PY{n}{anadir}\PY{p}{(}\PY{p}{[}\PY{n}{extremoInferiorIntervalo}\PY{p}{,}\PY{n}{extremoSuperiorIntervalo}\PY{p}{]}\PY{p}{,}
                            \PY{n}{minimoEnRonda}\PY{p}{,}
                            \PY{n}{interpolacion}\PY{p}{,}
                            \PY{n}{estadisticasDeSolucionesEnPDC}\PY{p}{,}
                            \PY{n}{tiemposTomados}\PY{p}{,}
                            \PY{n}{valores}\PY{p}{)}



    \PY{c+c1}{\PYZsh{} Reducción del intervalo de busqueda a un 20\PYZpc{} para la siguiente ronda de optimización.}
    \PY{n}{extremoInferiorIntervalo} \PY{o}{=} \PY{n}{minimoEnRonda} \PY{o}{*} \PY{l+m+mf}{0.9}
    \PY{n}{extremoSuperiorIntervalo} \PY{o}{=} \PY{n}{minimoEnRonda} \PY{o}{*}  \PY{l+m+mf}{1.1}
\end{Verbatim}
\end{tcolorbox}

    \begin{Verbatim}[commandchars=\\\{\}]
5.6730265617370605 1160
    \end{Verbatim}

    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Final_files/Final_34_3.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    Una de las conclusiones encontradas es que no se puede utilizar el ciclo
de optimización propuesto porque el ruido no permite que la
interpolación sea realista.

    \hypertarget{comparaciuxf3n-contra-la-soluciuxf3n-no-optimizada-y-la-soluciuxf3n-a-partir-de-puntos-aleatorios}{%
\subsection{Comparación contra la solución no optimizada y la solución a
partir de puntos
aleatorios}\label{comparaciuxf3n-contra-la-soluciuxf3n-no-optimizada-y-la-soluciuxf3n-a-partir-de-puntos-aleatorios}}

Para la comparación contra la solución no optimizada se va a observar la
convergencia en tiempo de cada función. Para esto se utilizaron
múltiples iteraciones de las soluciones con los parametros optimizados y
no optimizados. A continuación se verá el resultado

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{ }{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]

\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{21}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{k}{class} \PY{n+nc}{ultimaComparacion}\PY{p}{(}\PY{p}{)}\PY{p}{:}
    \PY{k}{def} \PY{n+nf+fm}{\PYZus{}\PYZus{}init\PYZus{}\PYZus{}}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,}\PY{n}{tiempos}\PY{p}{,}\PY{n}{perdidas}\PY{p}{,}\PY{n}{promedios}\PY{p}{,}\PY{n}{nombresito}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{tiempos} \PY{o}{=} \PY{n}{tiempos}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{registrosPerdidasGlobales} \PY{o}{=} \PY{n}{perdidas}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{registroPromedios} \PY{o}{=} \PY{n}{promedios}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{nombre} \PY{o}{=} \PY{n}{nombresito}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{frecuenciaSampleo} \PY{o}{=} \PY{l+m+mi}{200} \PY{c+c1}{\PYZsh{} Sampleo cada 200 datos}
        \PY{k}{pass}
    \PY{k}{def} \PY{n+nf}{anadirDato}\PY{p}{(}\PY{n+nb+bp}{self}\PY{p}{,}\PY{n}{tiempo}\PY{p}{,}\PY{n}{perdida}\PY{p}{)}\PY{p}{:}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{tiempos}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{tiempo}\PY{p}{)}
        \PY{n+nb+bp}{self}\PY{o}{.}\PY{n}{registrosPerdidasGlobales}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{perdida}\PY{p}{)}
        
\PY{n}{resultados} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{n}{resultados\PYZus{}dinamicos\PYZus{}agresivos} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{n}{resultados\PYZus{}dinamicos\PYZus{}conservadores} \PY{o}{=} \PY{p}{[}\PY{p}{]}
\PY{n}{directorio} \PY{o}{=} \PY{l+s+s2}{\PYZdq{}}\PY{l+s+s2}{/home/externo/Documents/nico/efficientPINN/pythontesis/v6/RESULTADOS/comparacionDeTiempo/terminados}\PY{l+s+s2}{\PYZdq{}}
\PY{k}{for} \PY{n}{filename} \PY{o+ow}{in} \PY{n}{os}\PY{o}{.}\PY{n}{listdir}\PY{p}{(}\PY{n}{directorio}\PY{p}{)}\PY{p}{:}
    \PY{k}{if} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{soluciones\PYZus{}uniformes.tar}\PY{l+s+s1}{\PYZsq{}} \PY{o+ow}{in} \PY{n}{filename}\PY{p}{:}
        \PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{directorio}\PY{p}{,}\PY{n}{filename}\PY{p}{)}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rb}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{as} \PY{n}{file}\PY{p}{:}
            \PY{n}{a} \PY{o}{=} \PY{n}{pickle}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{n}{file}\PY{p}{)}
            \PY{n}{file}\PY{o}{.}\PY{n}{close}\PY{p}{(}\PY{p}{)}
        \PY{k}{for} \PY{n}{solucion} \PY{o+ow}{in} \PY{n}{a}\PY{p}{:}
            \PY{n}{resultados}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{solucion}\PY{p}{)}
    \PY{k}{if} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{soluciones\PYZus{}dinamicas.tar}\PY{l+s+s1}{\PYZsq{}} \PY{o+ow}{in} \PY{n}{filename}\PY{p}{:}
        \PY{k}{with} \PY{n+nb}{open}\PY{p}{(}\PY{n}{os}\PY{o}{.}\PY{n}{path}\PY{o}{.}\PY{n}{join}\PY{p}{(}\PY{n}{directorio}\PY{p}{,}\PY{n}{filename}\PY{p}{)}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{rb}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)} \PY{k}{as} \PY{n}{file}\PY{p}{:}
            \PY{n}{a} \PY{o}{=} \PY{n}{pickle}\PY{o}{.}\PY{n}{load}\PY{p}{(}\PY{n}{file}\PY{p}{)}
            \PY{n}{file}\PY{o}{.}\PY{n}{close}\PY{p}{(}\PY{p}{)}
        \PY{k}{for} \PY{n}{solucion} \PY{o+ow}{in} \PY{n}{a}\PY{p}{:}
            \PY{k}{if} \PY{n}{filename}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{5}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
                \PY{n}{resultados\PYZus{}dinamicos\PYZus{}agresivos}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{solucion}\PY{p}{)}
            \PY{k}{if} \PY{n}{filename}\PY{o}{.}\PY{n}{split}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{\PYZhy{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}\PY{p}{[}\PY{l+m+mi}{4}\PY{p}{]} \PY{o}{==} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{30}\PY{l+s+s1}{\PYZsq{}}\PY{p}{:}
                \PY{n}{resultados\PYZus{}dinamicos\PYZus{}conservadores}\PY{o}{.}\PY{n}{append}\PY{p}{(}\PY{n}{solucion}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{30}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{contador} \PY{o}{=} \PY{l+m+mi}{0}
\PY{n}{colores} \PY{o}{=} \PY{p}{[}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{green}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{orange}\PY{l+s+s1}{\PYZsq{}}\PY{p}{]}

\PY{n}{fig}\PY{p}{,} \PY{n}{ax} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{subplots}\PY{p}{(}\PY{n}{figsize} \PY{o}{=} \PY{p}{(}\PY{l+m+mi}{12}\PY{p}{,}\PY{l+m+mi}{9}\PY{p}{)}\PY{p}{)}

\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{resultados\PYZus{}dinamicos\PYZus{}agresivos}\PY{p}{:}
    \PY{n}{ej} \PY{o}{=} \PY{n}{i}
    \PY{n}{ax}\PY{o}{.}\PY{n}{semilogy}\PY{p}{(}\PY{n}{ej}\PY{o}{.}\PY{n}{tiempos}\PY{p}{,}\PY{n}{filtrador}\PY{p}{(}\PY{n}{ej}\PY{o}{.}\PY{n}{registrosPerdidasGlobales}\PY{p}{)}\PY{p}{,} \PY{n}{color} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{blue}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{solucionador optimizado}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{resultados\PYZus{}dinamicos\PYZus{}conservadores}\PY{p}{:}
    \PY{n}{ej} \PY{o}{=} \PY{n}{i}
    \PY{n}{ax}\PY{o}{.}\PY{n}{semilogy}\PY{p}{(}\PY{n}{ej}\PY{o}{.}\PY{n}{tiempos}\PY{p}{,}\PY{n}{filtrador}\PY{p}{(}\PY{n}{ej}\PY{o}{.}\PY{n}{registrosPerdidasGlobales}\PY{p}{)}\PY{p}{,} \PY{n}{color} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{orange}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{solucionador no optimizado}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{resultados}\PY{p}{:}
    \PY{n}{ej} \PY{o}{=} \PY{n}{i}
    \PY{n}{ax}\PY{o}{.}\PY{n}{semilogy}\PY{p}{(}\PY{n}{ej}\PY{o}{.}\PY{n}{tiempos}\PY{p}{,}\PY{n}{filtrador}\PY{p}{(}\PY{n}{ej}\PY{o}{.}\PY{n}{registrosPerdidasGlobales}\PY{p}{)}\PY{p}{,} \PY{n}{color} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{green}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{solucionador sin algóritmo}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Evolución de los residuales para los distintos solucionadores}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Residuales}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{ax}\PY{o}{.}\PY{n}{set\PYZus{}xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{tiempo [s]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{handles}\PY{p}{,} \PY{n}{labels} \PY{o}{=} \PY{n}{ax}\PY{o}{.}\PY{n}{get\PYZus{}legend\PYZus{}handles\PYZus{}labels}\PY{p}{(}\PY{p}{)}
\PY{n}{display} \PY{o}{=} \PY{p}{[}\PY{n+nb}{len}\PY{p}{(}\PY{n}{resultados\PYZus{}dinamicos\PYZus{}agresivos}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{resultados\PYZus{}dinamicos\PYZus{}agresivos}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{o}{+}\PY{n+nb}{len}\PY{p}{(}\PY{n}{resultados\PYZus{}dinamicos\PYZus{}conservadores}\PY{p}{)}\PY{p}{,}\PY{n+nb}{len}\PY{p}{(}\PY{n}{handles}\PY{p}{)}\PY{o}{\PYZhy{}}\PY{l+m+mi}{2}\PY{p}{]}
\PY{n}{ax}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{[}\PY{n}{handle} \PY{k}{for} \PY{n}{i}\PY{p}{,}\PY{n}{handle} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{handles}\PY{p}{)} \PY{k}{if} \PY{n}{i} \PY{o+ow}{in} \PY{n}{display}\PY{p}{]}\PY{p}{,}
      \PY{p}{[}\PY{n}{label} \PY{k}{for} \PY{n}{i}\PY{p}{,}\PY{n}{label} \PY{o+ow}{in} \PY{n+nb}{enumerate}\PY{p}{(}\PY{n}{labels}\PY{p}{)} \PY{k}{if} \PY{n}{i} \PY{o+ow}{in} \PY{n}{display}\PY{p}{]}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{30}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
<matplotlib.legend.Legend at 0x7ff89dc74520>
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Final_files/Final_39_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    \hypertarget{section}{%
\subsubsection{}\label{section}}

A partir de la gráfica anterior, se puede observar claramente como el
algoritmo optimizado en promedio es más rápido que el algoritmo no
optimizado. Sin embargo, no es claro que este sea en promedio más rápido
que el algoritmo que propone la literatura. Consecuentemente se va a
comparar su rendimiento a partir de un Histograma

    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{46}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{n}{tiempos} \PY{o}{=} \PY{p}{[}\PY{n}{i}\PY{o}{.}\PY{n}{tiempos}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{resultados}\PY{p}{]}
\PY{n}{tiempos\PYZus{}dinamicos} \PY{o}{=} \PY{p}{[}\PY{n}{i}\PY{o}{.}\PY{n}{tiempos}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{resultados\PYZus{}dinamicos\PYZus{}agresivos}\PY{p}{]}
\PY{n}{tiempos\PYZus{}dinamicos\PYZus{}conservadores} \PY{o}{=} \PY{p}{[}\PY{n}{i}\PY{o}{.}\PY{n}{tiempos}\PY{p}{[}\PY{o}{\PYZhy{}}\PY{l+m+mi}{1}\PY{p}{]} \PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n}{resultados\PYZus{}dinamicos\PYZus{}conservadores}\PY{p}{]}

\PY{n}{n}\PY{p}{,} \PY{n}{bins}\PY{p}{,} \PY{n}{inutil} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{tiempos}\PY{p}{,}\PY{n}{density} \PY{o}{=} \PY{k+kc}{True}\PY{p}{,}\PY{n}{bins} \PY{o}{=} \PY{l+m+mi}{6}\PY{p}{,}\PY{n}{edgecolor} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{white}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{alpha} \PY{o}{=} \PY{l+m+mf}{0.5}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Tiempos de convergencia sin algoritmo}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{n}\PY{p}{)}\PY{p}{)}\PY{p}{:}
    \PY{n}{centro} \PY{o}{=} \PY{p}{(}\PY{n}{bins}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{+}\PY{n}{bins}\PY{p}{[}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{2}
    \PY{n}{plt}\PY{o}{.}\PY{n}{text}\PY{p}{(}\PY{n}{bins}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,}\PY{n}{n}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{    }\PY{l+s+si}{\PYZob{}}\PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{centro}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{tiempos}\PY{p}{)}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{tiempos}\PY{p}{)}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{n+nb}{max}\PY{p}{(}\PY{n}{n}\PY{p}{)}\PY{p}{,}\PY{n+nb}{min}\PY{p}{(}\PY{n}{n}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,}\PY{n}{marker}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{|}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{color} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{black}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Promedio tiempos sin algoritmo}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}


\PY{n}{n}\PY{p}{,} \PY{n}{bins}\PY{p}{,} \PY{n}{inutil} \PY{o}{=} \PY{n}{plt}\PY{o}{.}\PY{n}{hist}\PY{p}{(}\PY{n}{tiempos\PYZus{}dinamicos}\PY{p}{,}\PY{n}{density}\PY{o}{=}\PY{k+kc}{True}\PY{p}{,}\PY{n}{bins} \PY{o}{=} \PY{l+m+mi}{6}\PY{p}{,} \PY{n}{color} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{orange}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{edgecolor} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{white}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{alpha} \PY{o}{=} \PY{l+m+mf}{0.5}\PY{p}{,} \PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Tiempos de convergencia con algoritmo}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{grid}\PY{p}{(}\PY{n}{axis}\PY{o}{=}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{y}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}

\PY{k}{for} \PY{n}{i} \PY{o+ow}{in} \PY{n+nb}{range}\PY{p}{(}\PY{n+nb}{len}\PY{p}{(}\PY{n}{n}\PY{p}{)}\PY{p}{)}\PY{p}{:}
    \PY{n}{centro} \PY{o}{=} \PY{p}{(}\PY{n}{bins}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{o}{+}\PY{n}{bins}\PY{p}{[}\PY{n}{i}\PY{o}{+}\PY{l+m+mi}{1}\PY{p}{]}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{2}
    \PY{n}{plt}\PY{o}{.}\PY{n}{text}\PY{p}{(}\PY{n}{bins}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,}\PY{n}{n}\PY{p}{[}\PY{n}{i}\PY{p}{]}\PY{p}{,}\PY{l+s+sa}{f}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{    }\PY{l+s+si}{\PYZob{}}\PY{n}{np}\PY{o}{.}\PY{n}{round}\PY{p}{(}\PY{n}{centro}\PY{p}{,}\PY{l+m+mi}{0}\PY{p}{)}\PY{l+s+si}{\PYZcb{}}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{p}{)}

\PY{n}{plt}\PY{o}{.}\PY{n}{plot}\PY{p}{(}\PY{p}{[}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{tiempos\PYZus{}dinamicos}\PY{p}{)}\PY{p}{,}\PY{n}{np}\PY{o}{.}\PY{n}{mean}\PY{p}{(}\PY{n}{tiempos\PYZus{}dinamicos}\PY{p}{)}\PY{p}{]}\PY{p}{,}\PY{p}{[}\PY{n+nb}{max}\PY{p}{(}\PY{n}{n}\PY{p}{)}\PY{p}{,}\PY{n+nb}{min}\PY{p}{(}\PY{n}{n}\PY{p}{)}\PY{o}{/}\PY{l+m+mi}{2}\PY{p}{]}\PY{p}{,}\PY{n}{marker}\PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{|}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,} \PY{n}{color} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{orange}\PY{l+s+s1}{\PYZsq{}}\PY{p}{,}\PY{n}{label} \PY{o}{=} \PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Promedio tiempos sin algoritmo}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{legend}\PY{p}{(}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{title}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Histograma de tiempos para convergencia para cada caso}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{ylabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Frecuencia}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\PY{n}{plt}\PY{o}{.}\PY{n}{xlabel}\PY{p}{(}\PY{l+s+s1}{\PYZsq{}}\PY{l+s+s1}{Tiempo [s]}\PY{l+s+s1}{\PYZsq{}}\PY{p}{)}
\end{Verbatim}
\end{tcolorbox}

            \begin{tcolorbox}[breakable, size=fbox, boxrule=.5pt, pad at break*=1mm, opacityfill=0]
\prompt{Out}{outcolor}{46}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
Text(0.5, 0, 'Tiempo [s]')
\end{Verbatim}
\end{tcolorbox}
        
    \begin{center}
    \adjustimage{max size={0.9\linewidth}{0.9\paperheight}}{Final_files/Final_41_1.png}
    \end{center}
    { \hspace*{\fill} \\}
    
    A partir de la gráfica se puede observar una clara mejora en tiempo para
la convergencia tomado por el algoritmo. Para entender esto, se puede
analizar esta diferencia desde el tiempo minimo, el tiempo maximo y el
tiempo promedio.

Como es de esperar, el tiempo minimo no es significativamente diferente
al del algoritmo no optimizado. Esto se debe a que si por cuestiones de
suerte, siempre se está optimizando sobre los puntos adecuados, el
método convencional debería ir a una velocidad similar a la del
algoritmo propuesto. Lo anterior ocurre porque el objetivo del algoritmo
es mejorar la selección de puntos, lo que no necesariamente previene que
el método convencional pase por los puntos adecuados.

Por otra parte, en el tiempo máximo para la convergencia si se
encuentran mejoras significativas. El tiempo máximo tomado por el
algoritmo propuesto es un 28.3\% menor. Adicionalmente, el tiempo
promedio disminuye en un 25.4\%. Consecuentemente, se puede asumir que
el algoritmo conlleva a mejoras significativas en tiempo de
convergencia.

    \hypertarget{conclusiones}{%
\subsection{Conclusiones}\label{conclusiones}}

Se puede observar en las gráficas obtenidas a partir de la sección de
comparaciones , que las redes neuronales convergen a mayor velocidad
cuando se utiliza el algoritmo propuesto. Esto era esperable puesto que
este algoritmo previene el uso innecesario de puntos en areas donde no
se esta convergiendo a la solución deseada (Como es explicado en la
seccion X, estos puntos pueden estar convergiendo a una función que no
necesariamente es la función deseada). Adicionalmente, se están usando
funciones hechas en python, es decir que en python estos corren
significativamente más lento que las funciones nativas de pytorch. Se
considera que si se usara un lenguaje de menor nivel donde esto no fuese
un problema (como C, Golang o Rust), se podría esperar una mayor
discrepancia en la velocidad de las soluciones. Desafortunadamente la
implementación de esto escapa del alcance de este documento.

\hypertarget{conclusiones-1}{%
\subsection{Conclusiones}\label{conclusiones-1}}

\begin{itemize}
\tightlist
\item
  Se resolvió la ecuación diferencial planteada usando redes neuronales.
\item
  Se implementaron funciones de probabilidad dinámicas en el proceso de
  solución
\end{itemize}

    \hypertarget{bibliografuxeda}{%
\subsection{Bibliografía}\label{bibliografuxeda}}

    


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
